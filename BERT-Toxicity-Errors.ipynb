{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"BERT-Toxicity-Errors.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"yzLYci5nJTrV"},"source":["## My method"]},{"cell_type":"code","metadata":{"id":"MEpiduTgGGfe"},"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","dataset_dir = '/content/drive/My Drive/Incivility/Perspective/Perspective_BERT/datasets/'\n","model_save_dir = '/content/drive/My Drive/Incivility/Perspective/Perspective_BERT/model/'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6pHwc_RwGozL"},"source":["!pip install transformers==2.6.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ENOTHXTBGC1u"},"source":["import numpy as np\n","import torch\n","import transformers\n","from tqdm import tqdm, trange\n","from nltk import word_tokenize, sent_tokenize\n","\n","from torch.utils import data\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, WeightedRandomSampler\n","from transformers import BertTokenizer, BertConfig\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","from transformers import BertForTokenClassification, AdamW, BertModel, BertForSequenceClassification\n","from transformers import get_linear_schedule_with_warmup\n","\n","from sklearn.metrics import f1_score, accuracy_score, classification_report\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.model_selection import KFold, StratifiedKFold # if needed later\n","from torch.nn import CrossEntropyLoss, MSELoss \n","\n","import csv\n","\n","import pandas\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oJ2DIpi2Gs6o"},"source":["# Function to tokenize sentences for BERT\n","def tokenize_and_preserve(sentence, tokenizer):\n","\ttokenized_sentence = []\n","\n","\tfor word in sentence:\n","\n","\t\t# Tokenize the word and count # of subwords the word is broken into\n","\t\ttokenized_word = tokenizer.tokenize(word)\n","\t\tn_subwords = len(tokenized_word)\n","\n","\t\t# Add the tokenized word to the final tokenized word list\n","\t\ttokenized_sentence.extend(tokenized_word)\n","\n","\treturn tokenized_sentence\n","\n","# Load BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dsODamZzGgaS"},"source":["class CivilityRegressor(torch.nn.Module):\n","  def __init__(self):\n","    super(CivilityRegressor, self).__init__()\n","    # Need to freeze this, and learn weights on other layers\n","    # self.bert = BertModel.from_pretrained(\"bert-base-uncased\", output_attentions=False)\n","    # set_parameter_requires_grad(self.bert, feature_extracting=False)\n","    # self.drop = torch.nn.Dropout(p=0.3)\n","    # self.out = torch.nn.Linear(768, 1)\n","\n","    self.bert = BertForSequenceClassification.from_pretrained(\n","        \"bert-base-cased\", # Use the 12-layer BERT model, with an uncased vocab.\n","        num_labels = 2, # The number of output labels--2 for binary classification.\n","                        # You can increase this for multi-class tasks. \n","        # num_labels = 1,  \n","        output_attentions = False, # Whether the model returns attentions weights.\n","        output_hidden_states = False, # Whether the model returns all hidden-states.\n","    )\n","    # Freeze the BERT model\n","    \n","    for name, param in self.bert.named_parameters():\n","        if 'classifier' not in name:\n","            param.requires_grad = False\n","        else:\n","            print(name)\n","  \n","  def forward(self, input_ids, attention_mask, labels):\n","    # _, pooled_output, attention = self.bert(\n","    #   input_ids=input_ids,\n","    #   attention_mask=attention_mask\n","    # )\n","    # output = self.drop(pooled_output)\n","    # return self.out(features)\n","    \n","    loss, logits = self.bert(\n","      input_ids=input_ids,\n","      attention_mask=attention_mask,\n","      labels=labels\n","    )\n","    return loss, logits\n","\n","\n","model = CivilityRegressor()\n","print(model.bert.classifier.weight)\n","model.cuda();\n","checkpoint = torch.load('{}model.pt'.format(model_save_dir+'Classification_first/'))\n","model.load_state_dict(checkpoint)\n","print(model.bert.classifier.weight)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WTW9dp8Z0F1d"},"source":["print(model.bert.classifier.weight)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b_mtENyNGJD1"},"source":["model.eval()\n","input_file_1 = '/content/drive/My Drive/Backup/Research/Incivility/Perspective/Toxicity_Error_Analysis_Train_Set_1_100k.tsv'\n","\n","output_file_1 = 'Toxicity_Error_Analysis_BERT_Train_Set_3_80k.tsv'\n","\n","with open(input_file_1) as f, open(output_file_1, 'w') as w:\n","    header = f.readline()\n","    reader = csv.reader(f, delimiter=\"\\t\")\n","    writer = csv.writer(w)\n","    for i, row in enumerate(reader):\n","\n","        if i%100 == 0:\n","            print(i)\n","        new_row = list(filter(('').__ne__, row))[:5]\n","        if len(new_row) != 5:\n","            print(new_row[0])\n","        text = new_row[1]\n","\n","        # Tokenize sentences\n","        tokenized_text = tokenize_and_preserve(text, tokenizer)\n","        # Get IDs for inputs and perform padding\n","        input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(tokenized_text)],\n","            maxlen=256, dtype=\"long\", value=0.0,\n","            truncating=\"post\", padding=\"post\")\n","        # print(input_ids.dtype)\n","\n","        # Obtain labels, scores and attention masks\n","        attention_masks = np.array([float(i != 0.0) for i in input_ids[0]])\n","\n","        with torch.no_grad():\n","            # Forward pass, calculate logit predictions.\n","            loss, logits = model(torch.tensor(input_ids[0]).view(1, -1).cuda(), attention_mask=torch.tensor(attention_masks).view(1, -1).cuda(), labels=torch.ones((1, 1), dtype=torch.long).cuda())\n","\n","        # Move logits and labels to CPU\n","        scores = logits[:, 1].detach().cpu().numpy()\n","        # prob = torch.sigmoid(logits)\n","        # scores = prob[:, 1]\n","        new_row.append(scores.item())\n","\n","        writer.writerow(new_row)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kZ8pxNufJWX8"},"source":["## Other method"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LrhD5qPHNxSW","executionInfo":{"status":"ok","timestamp":1620317078760,"user_tz":240,"elapsed":36213,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}},"outputId":"ec9ef898-bd0d-4105-ef9f-3c7f8d5d3737"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0qbAjQsYK9sg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620317085334,"user_tz":240,"elapsed":42779,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}},"outputId":"6e9dbd3e-bbff-4659-d7c2-5cc19d79de9a"},"source":["!pip install pytorch-pretrained-bert"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting pytorch-pretrained-bert\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n","\u001b[K     |████████████████████████████████| 133kB 8.3MB/s \n","\u001b[?25hCollecting boto3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/99/43e5571005c792284276986eabd956699fac65d283df409b1482ca8722d8/boto3-1.17.67-py2.py3-none-any.whl (131kB)\n","\u001b[K     |████████████████████████████████| 133kB 10.4MB/s \n","\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.8.1+cu101)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n","Collecting jmespath<1.0.0,>=0.7.1\n","  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n","Collecting botocore<1.21.0,>=1.20.67\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/2b/6a23d63e1b9593919fbe622596fe92e02e3abcec7d2f91594443ee6e4ef9/botocore-1.20.67-py2.py3-none-any.whl (7.5MB)\n","\u001b[K     |████████████████████████████████| 7.5MB 8.2MB/s \n","\u001b[?25hCollecting s3transfer<0.5.0,>=0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n","\u001b[K     |████████████████████████████████| 81kB 12.1MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.67->boto3->pytorch-pretrained-bert) (2.8.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.67->boto3->pytorch-pretrained-bert) (1.15.0)\n","\u001b[31mERROR: botocore 1.20.67 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n","Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n","Successfully installed boto3-1.17.67 botocore-1.20.67 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.4.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u7ocqCVPHNEe","executionInfo":{"status":"ok","timestamp":1620317088925,"user_tz":240,"elapsed":46369,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}}},"source":["from pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\n","from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification,BertAdam"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"9EJcUX0LHS5V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620317090517,"user_tz":240,"elapsed":47955,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}},"outputId":"b0cc480f-3dd7-4a76-b022-02c8101062ec"},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["100%|██████████| 231508/231508 [00:00<00:00, 881181.95B/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"2ERTbsXPhJ4S","executionInfo":{"status":"ok","timestamp":1620317090517,"user_tz":240,"elapsed":47954,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}}},"source":["# Converting the lines to BERT format\n","# Thanks to https://www.kaggle.com/httpwwwfszyc/bert-in-keras-taming\n","from tqdm import tqdm, tqdm_notebook\n","import numpy as np\n","def convert_lines(example, max_seq_length,tokenizer):\n","    max_seq_length -=2\n","    all_tokens = []\n","    longer = 0\n","    # for text in tqdm_notebook(example):\n","    for text in example:\n","        tokens_a = tokenizer.tokenize(text)\n","        if len(tokens_a)>max_seq_length:\n","            tokens_a = tokens_a[:max_seq_length]\n","            longer += 1\n","        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n","        all_tokens.append(one_token)\n","    # print(longer)\n","    return np.array(all_tokens)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"FdUOumFTnY0R","executionInfo":{"status":"ok","timestamp":1620317090517,"user_tz":240,"elapsed":47953,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}}},"source":["import torch\n","import torch.nn as nn\n","import torch.utils.data\n","import torch.nn.functional as F\n","import csv\n","# train_dataset = torch.utils.data.TensorDataset(torch.tensor(X,dtype=torch.long), torch.tensor(y,dtype=torch.float))"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"Knw8WMvzTD6h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620317120811,"user_tz":240,"elapsed":78243,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}},"outputId":"755da646-cd10-4d8a-d8c7-68dd61f59620"},"source":["#Re-load model from file\n","device=torch.device('cuda')\n","# path = \"/content/gdrive/My Drive/Incivility/Perspective/Perspective_BERT/bert_pytorch_0.5_larger.bin\"\n","path = \"/content/gdrive/My Drive/Incivility/Perspective/Perspective_BERT/Final/Data/bert_pytorch_0.5_thresh.bin\"\n","# path = \"/content/gdrive/My Drive/Incivility/Perspective/Perspective_BERT/Retrained/bert_pytorch_0.5_retrained.bin\"\n","y_columns=['target']\n","model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=len(y_columns))\n","model.load_state_dict(torch.load(path))\n","model.to(device)\n","MAX_SEQUENCE_LENGTH = 128"],"execution_count":7,"outputs":[{"output_type":"stream","text":["100%|██████████| 407873900/407873900 [00:10<00:00, 39354259.42B/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gV1lH814e4Q3","executionInfo":{"status":"ok","timestamp":1620317120812,"user_tz":240,"elapsed":78240,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}},"outputId":"0d84cb3e-e5a3-45a0-db47-60a702192c96"},"source":["for param in model.parameters():\n","    param.requires_grad=False\n","model.eval()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): BertLayerNorm()\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=1, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"WwByYgxquJgG"},"source":["# Overpredictions and underpredictions\n","\n","def make_clean_dataset(dataset):\n","    count = 0\n","    cleaned_dataset = []\n","    for idx, line in enumerate(dataset):\n","\n","        if idx % 100000 == 0:\n","            print(idx)\n","\n","        comment, remaining = line[1], line[2:]\n","\n","        if comment.count('\\t') > 3:    \n","            # print('\\n Trying to fix **', line)          \n","            new_lines = comment.split('\\n')\n","            \n","            first_row = new_lines[0].split('\\t')\n","            first_row.insert(0, line[0])\n","            cleaned_dataset.append(first_row)\n","            # print('^ ', first_row)\n","            for linex in new_lines[1:-1]:\n","                fields = linex.split('\\t')\n","                # print('& ', fields)\n","                cleaned_dataset.append(fields)\n","            \n","            last_row = new_lines[-1].split('\\t')\n","            last_row.extend(remaining)\n","            cleaned_dataset.append(last_row)\n","            # print('$ ', last_row)\n","            count += len(new_lines)\n","            # break\n","        else:\n","            count += 1\n","            cleaned_dataset.append(line)\n","\n","    print(count, len(cleaned_dataset))\n","    return cleaned_dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wJroRSwxJ3IM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619830291048,"user_tz":240,"elapsed":1376884,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}},"outputId":"a897eb35-e4a7-4d41-d7dd-fe5c4ae19956"},"source":["input_file_1 = '/content/gdrive/My Drive/Backup/Research/Incivility/Perspective/Toxicity_Error_Analysis_Train_Set_1_100k.tsv'\n","output_file_1 = '/content/gdrive/My Drive/Backup/Research/Incivility/Perspective/Toxicity_Error_Analysis_BERT_Train_Set_1_Final_Retrained.tsv'\n","\n","dataset = []\n","with open(input_file_1) as f, open(output_file_1, 'w') as w:\n","    header = f.readline()\n","    reader = csv.reader(f, delimiter=\"\\t\")\n","    writer = csv.writer(w, delimiter=\"\\t\")\n","    for i, row in enumerate(reader):\n","\n","        if i%10000 == 0:\n","            print(i)\n","        \n","        # if i == 100:\n","        #     break\n","        new_row = list(filter(('').__ne__, row))[:5]\n","        # if len(new_row) != 5:\n","        #     print(new_row[0])\n","        dataset.append(new_row)\n","\n","    cleaned_dataset = make_clean_dataset(dataset)\n","\n","    for i, row in enumerate(cleaned_dataset):\n","        if i%10000 == 0:\n","            print(i)\n","\n","        text = row[1]\n","        label = float(row[2])\n","\n","        input = torch.tensor(convert_lines([text], MAX_SEQUENCE_LENGTH, tokenizer), dtype=torch.long)\n","        \n","        pred = model(input.to(device), attention_mask=(input>0).to(device), labels=None)\n","        score = torch.sigmoid(pred).item()\n","        \n","        error = label - score\n","\n","        row.append(score)\n","        row.append(error)\n","        writer.writerow(row)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0\n","10000\n","20000\n","30000\n","40000\n","50000\n","60000\n","70000\n","80000\n","90000\n","100000\n","0\n","100000\n","101596 101596\n","0\n","10000\n","20000\n","30000\n","40000\n","50000\n","60000\n","70000\n","80000\n","90000\n","100000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MpFwl9uA--5i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619831589943,"user_tz":240,"elapsed":2673788,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}},"outputId":"ae370dad-2142-4753-807a-ab3a778f1c9d"},"source":["input_file_2 = '/content/gdrive/My Drive/Backup/Research/Incivility/Perspective/Toxicity_Error_Analysis_Train_Set_2_100k.tsv'\n","output_file_2 = '/content/gdrive/My Drive/Backup/Research/Incivility/Perspective/Toxicity_Error_Analysis_BERT_Train_Set_2_Final_Retrained.tsv'\n","\n","dataset = []\n","with open(input_file_2) as f, open(output_file_2, 'w') as w:\n","    header = f.readline()\n","    reader = csv.reader(f, delimiter=\"\\t\")\n","    writer = csv.writer(w, delimiter=\"\\t\")\n","    for i, row in enumerate(reader):\n","\n","        if i%10000 == 0:\n","            print(i)\n","        \n","        # if i == 100:\n","        #     break\n","        new_row = list(filter(('').__ne__, row))[:5]\n","        # if len(new_row) != 5:\n","        #     print(new_row[0])\n","        dataset.append(new_row)\n","\n","    cleaned_dataset = make_clean_dataset(dataset)\n","\n","    for i, row in enumerate(cleaned_dataset):\n","        if i%10000 == 0:\n","            print(i)\n","\n","        text = row[1]\n","        label = float(row[2])\n","\n","        input = torch.tensor(convert_lines([text], MAX_SEQUENCE_LENGTH, tokenizer), dtype=torch.long)\n","        \n","        pred = model(input.to(device), attention_mask=(input>0).to(device), labels=None)\n","        score = torch.sigmoid(pred).item()\n","        \n","        error = label - score\n","\n","        row.append(score)\n","        row.append(error)\n","        writer.writerow(row)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0\n","10000\n","20000\n","30000\n","40000\n","50000\n","60000\n","70000\n","80000\n","90000\n","0\n","95510 95510\n","0\n","10000\n","20000\n","30000\n","40000\n","50000\n","60000\n","70000\n","80000\n","90000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FDcPFJOjt0p5","executionInfo":{"status":"ok","timestamp":1619833004150,"user_tz":240,"elapsed":4083476,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}},"outputId":"e3725fdb-c4b4-49a4-9658-c90c9c5b6983"},"source":["input_file_3 = '/content/gdrive/My Drive/Backup/Research/Incivility/Perspective/Toxicity_Error_Analysis_Train_Set_3_100k.tsv'\n","output_file_3 = '/content/gdrive/My Drive/Backup/Research/Incivility/Perspective/Toxicity_Error_Analysis_BERT_Train_Set_3_Final_Retrained.tsv'\n","\n","dataset = []\n","with open(input_file_3) as f, open(output_file_3, 'w') as w:\n","    header = f.readline()\n","    reader = csv.reader(f, delimiter=\"\\t\")\n","    writer = csv.writer(w, delimiter=\"\\t\")\n","    for i, row in enumerate(reader):\n","\n","        if i%10000 == 0:\n","            print(i)\n","        \n","        # if i == 100:\n","        #     break\n","        new_row = list(filter(('').__ne__, row))[:5]\n","        # if len(new_row) != 5:\n","        #     print(new_row[0])\n","        dataset.append(new_row)\n","\n","    cleaned_dataset = make_clean_dataset(dataset)\n","\n","    for i, row in enumerate(cleaned_dataset):\n","        if i%10000 == 0:\n","            print(i)\n","\n","        text = row[1]\n","        label = float(row[2])\n","\n","        input = torch.tensor(convert_lines([text], MAX_SEQUENCE_LENGTH, tokenizer), dtype=torch.long)\n","        \n","        pred = model(input.to(device), attention_mask=(input>0).to(device), labels=None)\n","        score = torch.sigmoid(pred).item()\n","        \n","        error = label - score\n","\n","        row.append(score)\n","        row.append(error)\n","        writer.writerow(row)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0\n","10000\n","20000\n","30000\n","40000\n","50000\n","60000\n","70000\n","80000\n","90000\n","100000\n","0\n","100000\n","103700 103700\n","0\n","10000\n","20000\n","30000\n","40000\n","50000\n","60000\n","70000\n","80000\n","90000\n","100000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HqI1nTEfGQTh"},"source":["## Template Analysis on BERT error triggers"]},{"cell_type":"code","metadata":{"id":"IZAV_0eBGyA9","executionInfo":{"status":"ok","timestamp":1620317121122,"user_tz":240,"elapsed":309,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}}},"source":["def get_score(text):\n","    input = torch.tensor(convert_lines([text], MAX_SEQUENCE_LENGTH, tokenizer), dtype=torch.long)\n","    pred = model(input.to(device), attention_mask=(input>0).to(device), labels=None)\n","    score = torch.sigmoid(pred).item()\n","    return score"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cd970jVfEkiR","executionInfo":{"status":"ok","timestamp":1620317125066,"user_tz":240,"elapsed":4230,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}},"outputId":"7a20d8fb-a83f-41a3-a4fa-61ba609bb51c"},"source":["import csv\n","# SCORES MIGHT SLIGHTLY DIFFER WHEN RUNNING AFTER FREEZE. SAME WITH ABOVE SCORES AND REGRESSION ON 300k. Ignore the \"_Retrained\" scores, \"_fixed\"\n","# templates = ['I am talking about _____', 'She did not say _____', 'We support _____']\n","templates = ['We wrote _____ on the page.','They whispered _____ to themselves. ','I erased the word _____ from the page.','You didn’t write _____.','Did he say _____ ?']\n","\n","# trigger_words = []\n","# with open('/content/gdrive/My Drive/Incivility/Perspective/Perspective_Error_Analysis/error_trigger_densities/all_errors/Regression_Words_BERT.csv') as f:\n","#     header = f.readline()\n","#     reader = csv.reader(f)\n","#     trigger_words = [line[0] for line in reader]\n","\n","# overprediction = trigger_words[-500:]\n","\n","# overprediction = ['woman', 'black', 'feminist', 'asian', 'blacks', 'males', 'migrants', 'female', 'christian', 'gays', 'homosexual', 'babies', 'african', 'dictator', 'mexicans', 'immigrants', 'girl', 'catholics', 'jewish', 'women', 'muslim', 'islamic', 'gay', 'muslims', 'jews']\n","\n","# overprediction = ['dictators', 'refugee', 'feminist', 'communist', 'catholic', 'migrants', 'homosexual', 'babies', 'mexicans', 'girl', 'islamic', 'woman', 'female', 'african', 'muslims', 'muslim', 'jews', 'blacks', 'black', 'jewish', 'gay', 'gays', 'women', 'dictator', 'catholics', 'police']\n","\n","overprediction = ['americans', 'baby', 'barack', 'boy', 'british', 'capitalist', 'catholic', 'child', 'chinese', 'communist', 'cop', 'cops', 'democrat', 'democrats', 'dictators', 'english', 'europeans', 'german', 'guy', 'hillary', 'irish', 'joseph', 'leftist', 'leftists', 'liberal', 'liberals', 'male', 'man', 'mexican', 'mom', 'mother', 'obama', 'police', 'potus', 'priests', 'progressives', 'putin', 'refugee', 'republicans', 'russians', 'tribal', 'trudeau']\n","\n","with open('/content/gdrive/My Drive/Incivility/Perspective/Perspective_Error_Analysis/error_trigger_densities/all_errors/Templates_BERT_persons_leftover.csv', 'w') as w:\n","    writer = csv.writer(w)\n","    writer.writerow(['error trigger', templates[0], templates[1], templates[2], templates[3], templates[4]])\n","    for i, word in enumerate(overprediction):\n","        t0 = templates[0].replace('_____', word)\n","        t1 = templates[1].replace('_____', word)\n","        t2 = templates[2].replace('_____', word)\n","        t3 = templates[3].replace('_____', word)\n","        t4 = templates[4].replace('_____', word)\n","        row = [word, get_score(t0), get_score(t1), get_score(t2), get_score(t3), get_score(t4)]\n","        writer.writerow(row)\n","        print(i+1, word)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["1 americans\n","2 baby\n","3 barack\n","4 boy\n","5 british\n","6 capitalist\n","7 catholic\n","8 child\n","9 chinese\n","10 communist\n","11 cop\n","12 cops\n","13 democrat\n","14 democrats\n","15 dictators\n","16 english\n","17 europeans\n","18 german\n","19 guy\n","20 hillary\n","21 irish\n","22 joseph\n","23 leftist\n","24 leftists\n","25 liberal\n","26 liberals\n","27 male\n","28 man\n","29 mexican\n","30 mom\n","31 mother\n","32 obama\n","33 police\n","34 potus\n","35 priests\n","36 progressives\n","37 putin\n","38 refugee\n","39 republicans\n","40 russians\n","41 tribal\n","42 trudeau\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HQ1_yAveqoCx"},"source":["## News Shows data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FD6UpyF_qnjk","executionInfo":{"status":"ok","timestamp":1619824300017,"user_tz":240,"elapsed":4728,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}},"outputId":"fe680b99-f12d-4768-840c-86744f4c1327"},"source":["import csv\n","\n","with open('/content/gdrive/My Drive/Incivility/Perspective/Perspective_BERT/snippet_data.csv') as f:\n","    header = f.readline().split(',')\n","    reader = csv.reader(f)\n","    data = [line for line in reader]\n","\n","with open('/content/gdrive/My Drive/Incivility/Perspective/Perspective_BERT/snippet_data_BERT.csv', 'w') as w:\n","    writer = csv.writer(w)\n","    header = ['Clip', 'Text', 'Human score', 'Perspective score', 'BERT-toxicity']\n","    writer.writerow(header)\n","    \n","    for i, line in enumerate(data):\n","        score = get_score(line[1])\n","        line.append(score) \n","        writer.writerow(line)\n","        print(i+1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","67\n","68\n","69\n","70\n","71\n","72\n","73\n","74\n","75\n","76\n","77\n","78\n","79\n","80\n","81\n","82\n","83\n","84\n","85\n","86\n","87\n","88\n","89\n","90\n","91\n","92\n","93\n","94\n","95\n","96\n","97\n","98\n","99\n","100\n","101\n","102\n","103\n","104\n","105\n","106\n","107\n","108\n","109\n","110\n","111\n","112\n","113\n","114\n","115\n","116\n","117\n","118\n","119\n","120\n","121\n","122\n","123\n","124\n","125\n","126\n","127\n","128\n","129\n","130\n","131\n","132\n","133\n","134\n","135\n","136\n","137\n","138\n","139\n","140\n","141\n","142\n","143\n","144\n","145\n","146\n","147\n","148\n","149\n","150\n","151\n","152\n","153\n","154\n","155\n","156\n","157\n","158\n","159\n","160\n","161\n","162\n","163\n","164\n","165\n","166\n","167\n","168\n","169\n","170\n","171\n","172\n","173\n","174\n","175\n","176\n","177\n","178\n","179\n","180\n","181\n","182\n","183\n","184\n","185\n","186\n","187\n","188\n","189\n","190\n","191\n","192\n","193\n","194\n","195\n","196\n","197\n","198\n","199\n","200\n","201\n","202\n","203\n","204\n","205\n","206\n","207\n","208\n","209\n","210\n","211\n","212\n","213\n","214\n","215\n","216\n","217\n","218\n","219\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hxVGqdHMSQ3j"},"source":["## Statictical significance for news data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PkzL0kZqSUqW","executionInfo":{"status":"ok","timestamp":1619824305043,"user_tz":240,"elapsed":434,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}},"outputId":"4104af64-3675-4b29-807e-13fc30bdc2bf"},"source":["'''\n","Script to compute statistical significance tests on the snippet level data (human and Perspective rated)\n","'''\n","import csv\n","from scipy.stats import ttest_ind, mannwhitneyu\n","import numpy as np\n","import itertools \n","\n","scores = {'FOX': [], 'MSNBC': [], 'PBS': []}\n","\n","with open('/content/gdrive/My Drive/Incivility/Perspective/Perspective_BERT/snippet_data_BERT.csv') as f:\n","\theader = f.readline()\n","\treader = csv.reader(f)\n","\tdata = [row for row in reader]\n","\tfor row in data:\n","\t\tdata = np.array([float(row[2]), float(row[3]), float(row[4])])\n","\t\t\n","\t\tif 'FOX' in row[0]:\n","\t\t\tscores['FOX'].append(data)\n","\t\tif 'MSNBC' in row[0]:\n","\t\t\tscores['MSNBC'].append(data)\n","\t\tif 'PBS' in row[0]:\n","\t\t\tscores['PBS'].append(data)\n","\n","scores['FOX'] = np.array(scores['FOX'])\n","scores['MSNBC'] = np.array(scores['MSNBC'])\n","scores['PBS'] = np.array(scores['PBS'])\n","\n","pair_list = list(itertools.combinations(list(scores.keys()), 2))\n","\n","for show1, show2 in pair_list:\n","\tprint(show1, show2)\n","\n","\t# print('human video scores')\n","\t# # Z, p = ttest_ind(scores[show1][:, 0], scores[show2][:, 0])\n","\t# # print('T-test pvalue: '+str(p))\n","\n","\t# Z, p = mannwhitneyu(scores[show1][:, 0], scores[show2][:, 0])\n","\t# print('Mann Whitney U-test pvalue: '+str(p))\n","\n","\tprint('xxxxxxx\\n')\n","\n","\tprint('human text scores')\n","\t# Z, p = ttest_ind(scores[show1][:, 1], scores[show2][:, 1])\n","\t# print('T-test pvalue: '+str(p))\n","\n","\tZ, p = mannwhitneyu(scores[show1][:, 0], scores[show2][:, 0])\n","\tprint('Mann Whitney U-test pvalue: '+str(p))\n","\n","\tprint('xxxxxxx\\n')\n","\n","\tprint('perspective scores')\n","\t# Z, p = ttest_ind(scores[show1][:, 2], scores[show2][:, 2])\n","\t# print('T-test pvalue: '+str(p))\n","\n","\tZ, p = mannwhitneyu(scores[show1][:, 1], scores[show2][:, 1])\n","\tprint('Mann Whitney U-test pvalue: '+str(p))\n","\n","\tprint('xxxxxxx\\n')\n","\n","\t# print('offensive scores')\n","\t# Z, p = ttest_ind(scores[show1][:, 3], scores[show2][:, 3])\n","\t# print('T-test pvalue: '+str(p))\n","\n","\t# Z, p = mannwhitneyu(scores[show1][:, 3], scores[show2][:, 3])\n","\t# print('Mann Whitney U-test pvalue: '+str(p))\n","\n","\tprint('bert scores')\n","\t# Z, p = ttest_ind(scores[show1][:, 3], scores[show2][:, 3])\n","\t# print('T-test pvalue: '+str(p))\n","\n","\tZ, p = mannwhitneyu(scores[show1][:, 2], scores[show2][:, 2])\n","\tprint('Mann Whitney U-test pvalue: '+str(p))\n","\n","\n","\tprint('-----------------------------\\n')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["FOX MSNBC\n","xxxxxxx\n","\n","human text scores\n","Mann Whitney U-test pvalue: 2.7249663862860052e-11\n","xxxxxxx\n","\n","perspective scores\n","Mann Whitney U-test pvalue: 2.818086444959442e-10\n","xxxxxxx\n","\n","bert scores\n","Mann Whitney U-test pvalue: 6.421075694326938e-09\n","-----------------------------\n","\n","FOX PBS\n","xxxxxxx\n","\n","human text scores\n","Mann Whitney U-test pvalue: 1.9903528258122023e-22\n","xxxxxxx\n","\n","perspective scores\n","Mann Whitney U-test pvalue: 4.5386855861941757e-10\n","xxxxxxx\n","\n","bert scores\n","Mann Whitney U-test pvalue: 8.935122696519494e-15\n","-----------------------------\n","\n","MSNBC PBS\n","xxxxxxx\n","\n","human text scores\n","Mann Whitney U-test pvalue: 0.00040087598191538166\n","xxxxxxx\n","\n","perspective scores\n","Mann Whitney U-test pvalue: 0.2514202218568159\n","xxxxxxx\n","\n","bert scores\n","Mann Whitney U-test pvalue: 0.0536937210300465\n","-----------------------------\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5paSoxnfLvXA"},"source":["## News Shows Transcript Data Analysis"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1D2P8PYBH2gk","executionInfo":{"status":"ok","timestamp":1619844418567,"user_tz":240,"elapsed":98447,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}},"outputId":"a9d2976e-2826-4a5c-fc18-d1cef1f0a4c1"},"source":["import csv\n","import nltk\n","nltk.download('punkt')\n","\n","segments_file = '/content/gdrive/My Drive/Backup/Research/Incivility/Annotations_Feb_March/Scores/transcript_level_analysis/segments_and_shows/all_segments_feb.csv'\n","output_file = '/content/gdrive/My Drive/Incivility/Perspective/Perspective_BERT/transcript_data_BERT.csv'\n","with open(segments_file) as f, open(output_file, 'w') as w:\n","    header = f.readline()\n","    reader = csv.reader(f)\n","    writer = csv.writer(w)\n","    for row in reader:\n","        text = row[0]\n","        info = [row[2], row[3], row[4]]\n","        words = nltk.word_tokenize(text)\n","        if len(words) > 256:\n","            sets = []\n","            c = 0\n","            sentences = nltk.sent_tokenize(text)\n","            for sentence in sentences:\n","                c += len(nltk.word_tokenize(sentence))\n","                sets.append(sentence)\n","                if c > 256:\n","                    t = ' '.join(sets)\n","                    s = get_score(t)\n","                    new_row = [t, s]\n","                    new_row.extend(info)\n","                    writer.writerow(new_row)\n","                    sets = [sentence]\n","                    c = len(nltk.word_tokenize(sentence))\n","                    \n","            t = ' '.join(sets)\n","            s = get_score(t)\n","            new_row = [t, s]\n","            new_row.extend(info)\n","            writer.writerow(new_row)    \n","\n","        else:\n","            s = get_score(text)\n","            new_row = [text, s]\n","            new_row.extend(info)\n","            writer.writerow(new_row)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gjZAvYjdw9fl","executionInfo":{"status":"ok","timestamp":1619870195503,"user_tz":240,"elapsed":1026,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}},"outputId":"ab30c489-8bcb-414f-ea37-64c9e9c73375"},"source":["'''\n","Script to compute statistical significance tests on the snippet level data (human and Perspective rated)\n","'''\n","import csv\n","from scipy.stats import ttest_ind, mannwhitneyu\n","import numpy as np\n","import itertools \n","\n","scores = {'FOX': [], 'MSNBC': [], 'PBS': []}\n","\n","with open('/content/gdrive/My Drive/Incivility/Perspective/Perspective_BERT/transcript_data_BERT_counts.csv') as f:\n","\theader = f.readline()\n","\treader = csv.reader(f)\n","\tdata = [row for row in reader]\n","\tfor row in data:\n","\t\tdata = np.array([float(row[2])])\n","\t\t\n","\t\tif 'FOX' in row[1]:\n","\t\t\tscores['FOX'].append(data)\n","\t\tif 'MSNBC' in row[1]:\n","\t\t\tscores['MSNBC'].append(data)\n","\t\tif 'PBS' in row[1]:\n","\t\t\tscores['PBS'].append(data)\n","\n","scores['FOX'] = np.array(scores['FOX'])\n","scores['MSNBC'] = np.array(scores['MSNBC'])\n","scores['PBS'] = np.array(scores['PBS'])\n","\n","pair_list = list(itertools.combinations(list(scores.keys()), 2))\n","\n","for show1, show2 in pair_list:\n","\tprint(show1, show2)\n","\n","\t# print('human video scores')\n","\t# # Z, p = ttest_ind(scores[show1][:, 0], scores[show2][:, 0])\n","\t# # print('T-test pvalue: '+str(p))\n","\n","\t# Z, p = mannwhitneyu(scores[show1][:, 0], scores[show2][:, 0])\n","\t# print('Mann Whitney U-test pvalue: '+str(p))\n","\n","\tprint('xxxxxxx\\n')\n","\n","\tprint('bert scores')\n","\t# Z, p = ttest_ind(scores[show1][:, 3], scores[show2][:, 3])\n","\t# print('T-test pvalue: '+str(p))\n","\n","\tZ, p = mannwhitneyu(scores[show1][:, 0], scores[show2][:, 0])\n","\tprint('Mann Whitney U-test pvalue: '+str(p))\n","\n","\n","\tprint('-----------------------------\\n')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["FOX MSNBC\n","xxxxxxx\n","\n","bert scores\n","Mann Whitney U-test pvalue: 1.903007442283471e-05\n","-----------------------------\n","\n","FOX PBS\n","xxxxxxx\n","\n","bert scores\n","Mann Whitney U-test pvalue: 0.00018924520925346386\n","-----------------------------\n","\n","MSNBC PBS\n","xxxxxxx\n","\n","bert scores\n","Mann Whitney U-test pvalue: 0.14067705592605478\n","-----------------------------\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"MFhadeRkuAwQ"},"source":["## EACL Templates Analysis"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MTLcKtB1uxfj","executionInfo":{"status":"ok","timestamp":1619886904702,"user_tz":240,"elapsed":148401,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}},"outputId":"0f463d90-f971-4cef-deb0-411bc8fb733a"},"source":["import csv\n","\n","templates = ['We wrote _____ on the page.','They whispered _____ to themselves. ','I erased the word _____ from the page.','You didn’t write _____.','Did he say _____ ?']\n","\n","trigger_words = []\n","with open('/content/gdrive/My Drive/Backup/Research/Incivility/Annotations_Feb_March/Scores/transcript_level_analysis/words/words_analysis_clean.csv', encoding = \"ISO-8859-1\") as f:\n","    header = f.readline()\n","    reader = csv.reader(f)\n","    trigger_words = [line[0] for line in reader]\n","\n","with open('/content/gdrive/My Drive/Incivility/Perspective/Perspective_Error_Analysis/error_trigger_densities/all_errors/Templates_BERT_EACL.csv', 'w') as w:\n","    writer = csv.writer(w)\n","    writer.writerow(['error trigger', templates[0], templates[1], templates[2], templates[3], templates[4]])\n","    for i, word in enumerate(trigger_words):\n","        t0 = templates[0].replace('_____', word)\n","        t1 = templates[1].replace('_____', word)\n","        t2 = templates[2].replace('_____', word)\n","        t3 = templates[3].replace('_____', word)\n","        t4 = templates[4].replace('_____', word)\n","        row = [word, get_score(t0), get_score(t1), get_score(t2), get_score(t3), get_score(t4)]\n","        writer.writerow(row)\n","        print(i+1, word)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1 0\n","2 1.375\n","3 3\n","4 4\n","5 5.7\n","6 10\n","7 11\n","8 17\n","9 18\n","10 19\n","11 20\n","12 24\n","13 26\n","14 37\n","15 50\n","16 55\n","17 70\n","18 73\n","19 90\n","20 157\n","21 200\n","22 224\n","23 427\n","24 500\n","25 1000\n","26 1973\n","27 1984\n","28 2004\n","29 2011\n","30 2012\n","31 2014\n","32 2018\n","33 2019\n","34 2020\n","35 5400\n","36 16501\n","37 25954\n","38 180000\n","39 -\n","40 --\n","41 ,\n","42 :\n","43 !\n","44 ?\n","45 ...\n","46 '\n","47 Ò\n","48 (\n","49 )\n","50 [\n","51 ]\n","52 &\n","53 ``\n","54 $\n","55 10:00:00\n","56 115-pound\n","57 13th\n","58 1700s\n","59 1800s\n","60 1860s\n","61 1970s\n","62 25th\n","63 39-year-old\n","64 5-foot-tall\n","65 6:30\n","66 64-year-old\n","67 A\n","68 a\n","69 a.m.\n","70 ability\n","71 able\n","72 abortion\n","73 About\n","74 about\n","75 above\n","76 Absolutely\n","77 absolutely\n","78 abuse\n","79 abused\n","80 abusively\n","81 abysmal\n","82 Academy\n","83 accept\n","84 accepted\n","85 access\n","86 According\n","87 according\n","88 accosted\n","89 account\n","90 accounts\n","91 accuracy\n","92 accused\n","93 accusing\n","94 achieve\n","95 acknowledge\n","96 acknowledged\n","97 Acosta\n","98 acquaintances\n","99 across\n","100 Act\n","101 acting\n","102 action\n","103 active\n","104 activity\n","105 Actor\n","106 actor\n","107 actually\n","108 added\n","109 addition\n","110 address\n","111 ADL\n","112 administration\n","113 admire\n","114 admired\n","115 admonish\n","116 adult\n","117 advances\n","118 advice\n","119 advised\n","120 adviser\n","121 advisers\n","122 Advisory\n","123 affected\n","124 affirming\n","125 afford\n","126 Africa\n","127 African-Americans\n","128 after\n","129 afternoon\n","130 afterward\n","131 Again\n","132 again\n","133 against\n","134 age-old\n","135 agenda\n","136 agent\n","137 Agnew\n","138 ago\n","139 agree\n","140 agreement\n","141 ahead\n","142 aid\n","143 airplanes\n","144 aisle\n","145 al\n","146 Alcindor\n","147 Alex\n","148 Alexandria\n","149 Alice\n","150 aliens\n","151 ALL\n","152 All\n","153 all\n","154 allegation\n","155 alleged\n","156 allegedly\n","157 alleges\n","158 alleging\n","159 Allen\n","160 allies\n","161 allocates\n","162 allowed\n","163 allowing\n","164 almost\n","165 along\n","166 Also\n","167 also\n","168 alter\n","169 although\n","170 always\n","171 am\n","172 amazing\n","173 Amazon\n","174 ambushed\n","175 Amendment\n","176 America\n","177 American\n","178 Americans\n","179 Amna\n","180 among\n","181 AMY\n","182 Amy\n","183 an\n","184 AND\n","185 And\n","186 and\n","187 and/or\n","188 Andrew\n","189 Andy\n","190 Angel\n","191 Angeles\n","192 annihilation\n","193 announce\n","194 announced\n","195 announcement\n","196 annual\n","197 another\n","198 answers\n","199 anti-semite\n","200 anti-Semites\n","201 Anti-Trump\n","202 anti-Trump\n","203 anti-white\n","204 anti-women\n","205 Antonio\n","206 any\n","207 anybody\n","208 anyone\n","209 anything\n","210 apologized\n","211 apology\n","212 apparently\n","213 appeal\n","214 appeals\n","215 appear\n","216 appeared\n","217 appears\n","218 appointed\n","219 appointment\n","220 appreciate\n","221 appreciated\n","222 appropriate\n","223 approved\n","224 AR-15\n","225 Arabia\n","226 arbitrary\n","227 architecture\n","228 are\n","229 area\n","230 areas\n","231 argued\n","232 argument\n","233 arms\n","234 Arnold\n","235 around\n","236 Art\n","237 art\n","238 arts\n","239 Aryan\n","240 As\n","241 as\n","242 ASC\n","243 asinine\n","244 ask\n","245 asking\n","246 assassinated\n","247 assault\n","248 assaulted\n","249 assaulting\n","250 assessment\n","251 assistance\n","252 associate\n","253 associated\n","254 assurances\n","255 at\n","256 ation\n","257 attack\n","258 attention-seeking\n","259 attorney\n","260 attraction\n","261 Author\n","262 authorities\n","263 authority\n","264 authorizing\n","265 average\n","266 avoid\n","267 avoids\n","268 awarding\n","269 aware\n","270 away\n","271 baby\n","272 Back\n","273 back\n","274 bacon\n","275 bad\n","276 Bag\n","277 bald\n","278 ball\n","279 ballots\n","280 Baltimore\n","281 Bank\n","282 bank\n","283 banners\n","284 Barb\n","285 Barr\n","286 barrier\n","287 base\n","288 based\n","289 Basically\n","290 basically\n","291 bastard\n","292 BE\n","293 be\n","294 beaten\n","295 became\n","296 Because\n","297 because\n","298 become\n","299 been\n","300 before\n","301 BEGIN\n","302 begin\n","303 beginning\n","304 begins\n","305 begun\n","306 behalf\n","307 beheaded\n","308 behind\n","309 being\n","310 believe\n","311 believed\n","312 belly\n","313 benefit\n","314 Bernie\n","315 besmirchment\n","316 bet\n","317 better\n","318 between\n","319 Bezos\n","320 bias\n","321 Bibles\n","322 big\n","323 biggest\n","324 bigot\n","325 Bill\n","326 bill\n","327 billion\n","328 bills\n","329 bipartisan\n","330 birds\n","331 bishops\n","332 bit\n","333 Black\n","334 black\n","335 blackface\n","336 BlacKkKlansman\n","337 Blanchard\n","338 Blatantly\n","339 blatantly\n","340 blistering\n","341 Blow\n","342 blowing\n","343 blunt\n","344 Bob\n","345 bomber\n","346 Bongino\n","347 bono\n","348 Booker\n","349 Border\n","350 border\n","351 borders\n","352 both\n","353 bottom\n","354 Bottoms\n","355 bouncer\n","356 Bowl\n","357 bozo\n","358 Brady\n","359 brain\n","360 brand\n","361 brand-new\n","362 BREAK\n","363 break\n","364 breaking\n","365 bribery\n","366 briefed\n","367 briefing\n","368 briefings\n","369 bring\n","370 bringing\n","371 Britain\n","372 British\n","373 broadcast\n","374 Broaddrick\n","375 broke\n","376 broken\n","377 Brooks\n","378 Brought\n","379 brought\n","380 Brown\n","381 brown\n","382 Bruce\n","383 bully\n","384 bunch\n","385 burdens\n","386 business\n","387 busy\n","388 But\n","389 but\n","390 Butina\n","391 BUTLER\n","392 By\n","393 by\n","394 Byrd\n","395 C'mon\n","396 ca\n","397 cabinet\n","398 caliphate\n","399 Call\n","400 call\n","401 called\n","402 calling\n","403 Calmly\n","404 came\n","405 campaign\n","406 Campos-\n","407 Can\n","408 can\n","409 candidacy\n","410 Candidate\n","411 candidate\n","412 capacity\n","413 Capitol\n","414 Caracas\n","415 Cardinal\n","416 Care\n","417 care\n","418 cares\n","419 caricature\n","420 Carolina\n","421 carry\n","422 cars\n","423 cartels\n","424 case\n","425 cash\n","426 Catholic\n","427 Catholics\n","428 Caucus\n","429 cave\n","430 celebrated\n","431 celebration\n","432 Center\n","433 century\n","434 CEO\n","435 certain\n","436 Certainly\n","437 certainly\n","438 chair\n","439 Chairman\n","440 chairman\n","441 challenge\n","442 challenges\n","443 chamber\n","444 change\n","445 changed\n","446 changes\n","447 changing\n","448 channel\n","449 chanting\n","450 character\n","451 charge\n","452 charged\n","453 charges\n","454 Charlottesville\n","455 cheater\n","456 CHEERING\n","457 Cheney\n","458 Chicago\n","459 chick\n","460 chicken\n","461 Chief\n","462 chief\n","463 child\n","464 children\n","465 China\n","466 Chinese\n","467 chock-full\n","468 chocolate\n","469 Chris\n","470 Christian\n","471 Chucky\n","472 Church\n","473 church\n","474 CIA\n","475 cities\n","476 citizen\n","477 citizenship\n","478 claims\n","479 classmate\n","480 clear\n","481 Clearly\n","482 clergy\n","483 clerical\n","484 climate\n","485 cling\n","486 Clinton\n","487 Clinton-sycophants\n","488 CLIP\n","489 clock\n","490 close\n","491 closed\n","492 closer\n","493 CNN\n","494 Co-Creator\n","495 co-creator\n","496 Coast\n","497 Coca-Cola\n","498 Cohan\n","499 Cohen\n","500 Cohunes\n","501 coin\n","502 cojones\n","503 colleagues\n","504 collect\n","505 Collins\n","506 collusion\n","507 Colorado\n","508 columnist\n","509 come\n","510 comes\n","511 Comey\n","512 comforts\n","513 coming\n","514 comment\n","515 comments\n","516 COMMERCIAL\n","517 commercial\n","518 commission\n","519 committed\n","520 Committee\n","521 committee\n","522 committees\n","523 commonwealth\n","524 communications\n","525 communities\n","526 community\n","527 commuted\n","528 companies\n","529 company\n","530 comprehension\n","531 compromised\n","532 concern\n","533 concerns\n","534 conclusion\n","535 concrete\n","536 conduct\n","537 conference\n","538 confessional\n","539 confided\n","540 confirmed\n","541 conforming\n","542 Congress\n","543 congressional\n","544 Congressman\n","545 congressman\n","546 Congresswoman\n","547 connection\n","548 consensual\n","549 conservative\n","550 consider\n","551 considerable\n","552 consistencies\n","553 conspiracy\n","554 constitution\n","555 contact\n","556 contacts\n","557 Content\n","558 content\n","559 continually\n","560 continue\n","561 continued\n","562 contractual\n","563 contributor\n","564 control\n","565 controlled\n","566 conversation\n","567 conversations\n","568 COOPER\n","569 Cooper\n","570 cooperate\n","571 cooperating\n","572 cooperation\n","573 coordinated\n","574 coordination\n","575 copies\n","576 COPY\n","577 copy\n","578 Copyrig\n","579 copyrig\n","580 core\n","581 correct\n","582 correspond\n","583 correspondent\n","584 Cory\n","585 cost\n","586 costs\n","587 costume\n","588 Could\n","589 could\n","590 Counsel\n","591 counsel\n","592 counsel's\n","593 Counter-\n","594 counter-intelligence\n","595 countries\n","596 country\n","597 counts\n","598 County\n","599 coupled\n","600 course\n","601 courtroom\n","602 cover-up\n","603 Covington\n","604 cows\n","605 CQ-\n","606 CQ-Roll\n","607 crackers\n","608 crap\n","609 crawling\n","610 Crazy\n","611 crazy\n","612 create\n","613 created\n","614 creatures\n","615 cries\n","616 crime\n","617 crimes\n","618 criminal\n","619 crisis\n","620 critic\n","621 critical\n","622 criticism\n","623 Critics\n","624 crock\n","625 crossing\n","626 CROSSTALK\n","627 Crow\n","628 cruelty\n","629 crying\n","630 culpa\n","631 cultural\n","632 culture\n","633 Cummings\n","634 curious\n","635 current\n","636 curtailing\n","637 custody\n","638 custom-made\n","639 cycle\n","640 D\n","641 d\n","642 D.C\n","643 D.C.\n","644 Dads\n","645 Dakota\n","646 damn\n","647 Dan\n","648 DANIELS\n","649 dare\n","650 darn\n","651 Daryl\n","652 data\n","653 daughter\n","654 David\n","655 Davis\n","656 day\n","657 days\n","658 deadline\n","659 deadly\n","660 Deal\n","661 deal\n","662 death\n","663 debates\n","664 decade\n","665 decades\n","666 December\n","667 decent\n","668 decide\n","669 decided\n","670 decision\n","671 declaration\n","672 decrepit\n","673 deep\n","674 defendant\n","675 defendants\n","676 Defense\n","677 definitely\n","678 deflection\n","679 defy\n","680 degree\n","681 dehumanizing\n","682 delay\n","683 DELETED\n","684 Dem\n","685 demanding\n","686 democracy\n","687 Democrat\n","688 Democratic\n","689 Democrats\n","690 demonize\n","691 demonstrably\n","692 denied\n","693 denigrate\n","694 Denver\n","695 denying\n","696 Department\n","697 department\n","698 deplorables\n","699 depression\n","700 deputy\n","701 Derrick\n","702 described\n","703 deserts\n","704 design\n","705 Desjardins\n","706 despite\n","707 destroy-\n","708 detail\n","709 details\n","710 determine\n","711 deterrence\n","712 deters\n","713 Deutsche\n","714 develop\n","715 developing\n","716 development\n","717 Dick\n","718 Did\n","719 did\n","720 didn't\n","721 die\n","722 died\n","723 difference\n","724 different\n","725 difficult\n","726 dig\n","727 Dingell\n","728 Dinkins\n","729 diocese\n","730 direct\n","731 directed\n","732 direction\n","733 Director\n","734 dirty\n","735 dirtying\n","736 disclosure\n","737 Discovers\n","738 discussed\n","739 discussion\n","740 disgusting\n","741 displayed\n","742 disqualifiers\n","743 disrespectful\n","744 distortion\n","745 distributed\n","746 District\n","747 district\n","748 Dixon\n","749 Do\n","750 do\n","751 document\n","752 documentary\n","753 documents\n","754 Does\n","755 does\n","756 Doing\n","757 dollar\n","758 domestic\n","759 DONALD\n","760 Donald\n","761 Donaldson\n","762 donations\n","763 doomed\n","764 door\n","765 doors\n","766 dopest\n","767 dossier\n","768 Double\n","769 double\n","770 Douglas\n","771 doused\n","772 Dow\n","773 Dowless\n","774 down\n","775 draft\n","776 drama\n","777 driving\n","778 drop\n","779 dropped\n","780 drug\n","781 drugs\n","782 drumming\n","783 Drunk\n","784 ducking\n","785 due\n","786 Duffy\n","787 Dukakis\n","788 Duke\n","789 duly-elected\n","790 dumb\n","791 dumbest\n","792 dummy\n","793 dumped\n","794 dumping\n","795 duration\n","796 dusty\n","797 e\n","798 eagle\n","799 early\n","800 earth\n","801 easier\n","802 easily\n","803 economic\n","804 economy\n","805 edition\n","806 education\n","807 effect\n","808 Effective\n","809 effectively\n","810 effort\n","811 eggs\n","812 either\n","813 elected\n","814 election\n","815 elections\n","816 element\n","817 elements\n","818 elevating\n","819 Elijah\n","820 Eloquent\n","821 else\n","822 emergencies\n","823 emergency\n","824 Emmett\n","825 emotion\n","826 empires\n","827 encountered\n","828 END\n","829 end\n","830 engaged\n","831 engineered\n","832 Enquirer\n","833 enthusiasm\n","834 entities\n","835 environment\n","836 epidemic\n","837 Epstein\n","838 Eric\n","839 Erickson\n","840 errand\n","841 essentially\n","842 Europe\n","843 European\n","844 even\n","845 evening\n","846 eventual\n","847 eventually\n","848 ever\n","849 Every\n","850 every\n","851 everybody\n","852 everyone\n","853 Everything\n","854 everything\n","855 everywhere\n","856 evidence\n","857 evil\n","858 evolution\n","859 exact\n","860 exactly\n","861 example\n","862 exceeds\n","863 except\n","864 excess\n","865 excuse\n","866 executive\n","867 exist\n","868 expand\n","869 expectations\n","870 expected\n","871 expecting\n","872 expensive\n","873 experience\n","874 expert\n","875 explained\n","876 explode\n","877 exploit\n","878 expressed\n","879 extension\n","880 extent\n","881 extraordinary\n","882 extreme\n","883 extremist-related\n","884 eye\n","885 F-ing\n","886 F.\n","887 face\n","888 faces\n","889 facilities\n","890 facing\n","891 fact\n","892 factors\n","893 facts\n","894 Fairfax\n","895 fairly\n","896 faith\n","897 fake\n","898 fall\n","899 Fallon\n","900 FALSE\n","901 familiar\n","902 families\n","903 family\n","904 famous\n","905 far\n","906 farting\n","907 fashion\n","908 fawning\n","909 FBI\n","910 fearful\n","911 February\n","912 federal\n","913 feel\n","914 felonies\n","915 felony\n","916 felt\n","917 feminism\n","918 Feminist\n","919 fervently\n","920 few\n","921 field\n","922 fight\n","923 fighters\n","924 figures\n","925 filed\n","926 filing\n","927 filings\n","928 film\n","929 filmed\n","930 films\n","931 FINAL\n","932 final\n","933 finance\n","934 financial\n","935 find\n","936 finding\n","937 findings\n","938 finds\n","939 finish\n","940 firearm\n","941 fired\n","942 First\n","943 first\n","944 FISA\n","945 fithy\n","946 five\n","947 flags\n","948 Fleischer\n","949 Florida\n","950 flyering\n","951 flyers\n","952 Flynn\n","953 folks\n","954 food\n","955 fool\n","956 foot\n","957 foothold\n","958 footprint\n","959 For\n","960 for\n","961 force\n","962 forced\n","963 forces\n","964 Foreign\n","965 forever\n","966 forget\n","967 FORM\n","968 form\n","969 Former\n","970 FOX\n","971 Fox\n","972 FRANCIS\n","973 Francis\n","974 fraud\n","975 Fray\n","976 free\n","977 Friday\n","978 friend\n","979 friends\n","980 From\n","981 from\n","982 front\n","983 front-row\n","984 fry\n","985 fryers\n","986 funding\n","987 funds\n","988 future\n","989 g\n","990 gag\n","991 gagged\n","992 gang\n","993 garbage\n","994 garner\n","995 Gary\n","996 Gates\n","997 gauge\n","998 gay\n","999 gender\n","1000 General\n","1001 general\n","1002 generally\n","1003 Georg\n","1004 Georgia\n","1005 Geraldo\n","1006 get\n","1007 gets\n","1008 getting\n","1009 gifted\n","1010 gigantic\n","1011 girls\n","1012 give\n","1013 glad\n","1014 gloating\n","1015 global\n","1016 Go\n","1017 go\n","1018 goal\n","1019 God\n","1020 goes\n","1021 going\n","1022 Good\n","1023 good\n","1024 goons\n","1025 got\n","1026 government\n","1027 Governor\n","1028 governor\n","1029 governors\n","1030 governorship\n","1031 gradual\n","1032 Gradualism\n","1033 gradualism\n","1034 gradualists\n","1035 grand\n","1036 granny\n","1037 gratefulness\n","1038 gravitate\n","1039 Great\n","1040 great\n","1041 Green\n","1042 green\n","1043 Gregg\n","1044 grew\n","1045 group\n","1046 groups\n","1047 grow\n","1048 GRU\n","1049 Guaido\n","1050 Guard\n","1051 guess\n","1052 guest\n","1053 guidelines\n","1054 guilty\n","1055 gun\n","1056 guy\n","1057 guy's\n","1058 guys\n","1059 had\n","1060 halfway\n","1061 halling\n","1062 hand\n","1063 handed\n","1064 handful\n","1065 handgun\n","1066 handled\n","1067 HANNITY\n","1068 Hannity\n","1069 hannity.com\n","1070 happened\n","1071 happens\n","1072 happiness\n","1073 Happy\n","1074 harassed\n","1075 harboring\n","1076 Harris\n","1077 has\n","1078 Hasson\n","1079 hat\n","1080 hate\n","1081 hate-\n","1082 hate-Trump\n","1083 hatred\n","1084 Have\n","1085 have\n","1086 having\n","1087 HBO\n","1088 He\n","1089 he\n","1090 he'\n","1091 he's\n","1092 head\n","1093 heads\n","1094 Health\n","1095 health\n","1096 Healthcare\n","1097 healthy\n","1098 hear\n","1099 heard\n","1100 hearing\n","1101 heartless\n","1102 heaven\n","1103 heavy\n","1104 Hebrew\n","1105 Hegel\n","1106 Hello\n","1107 help\n","1108 helped\n","1109 helpful\n","1110 Her\n","1111 her\n","1112 Here\n","1113 here\n","1114 herein\n","1115 heroin\n","1116 herself\n","1117 high\n","1118 highest\n","1119 Hillary\n","1120 him\n","1121 His\n","1122 his\n","1123 historical\n","1124 history\n","1125 HIV\n","1126 hoax\n","1127 Holdings\n","1128 home\n","1129 Homophobia\n","1130 Homophobic\n","1131 homophobic\n","1132 honestly\n","1133 honeymoon\n","1134 honorably\n","1135 hope\n","1136 hoped\n","1137 hopes\n","1138 hormone\n","1139 horrors\n","1140 host\n","1141 hour\n","1142 House\n","1143 house\n","1144 housing\n","1145 How\n","1146 how\n","1147 However\n","1148 however\n","1149 ht\n","1150 hts\n","1151 huge\n","1152 human\n","1153 humanitarian\n","1154 hundred\n","1155 hush\n","1156 Hymie\n","1157 hypocrisy\n","1158 hypocrite\n","1159 I\n","1160 ICE\n","1161 idea\n","1162 ideas\n","1163 identified\n","1164 identify\n","1165 idiot\n","1166 idiotic\n","1167 If\n","1168 if\n","1169 ifs\n","1170 II\n","1171 illegal\n","1172 illness\n","1173 image\n","1174 immediately\n","1175 immigration\n","1176 immunity\n","1177 impact\n","1178 impeached\n","1179 imperialists\n","1180 implicated\n","1181 implications\n","1182 importance\n","1183 important\n","1184 impugning\n","1185 impunity\n","1186 IN\n","1187 In\n","1188 in\n","1189 inaudible\n","1190 inaugural\n","1191 Inc.\n","1192 incendiary\n","1193 incident\n","1194 include\n","1195 included\n","1196 including\n","1197 income\n","1198 inconclusive\n","1199 increase\n","1200 increasingly\n","1201 indeed\n","1202 independent\n","1203 India\n","1204 indict\n","1205 indicted\n","1206 indictment\n","1207 individuals\n","1208 industrial\n","1209 industry\n","1210 inequality\n","1211 inextricably\n","1212 INF\n","1213 infanticide\n","1214 infiltrated\n","1215 information\n","1216 infring\n","1217 infuriating\n","1218 inherently\n","1219 inhumane\n","1220 initially\n","1221 inmate\n","1222 innocence\n","1223 innocent\n","1224 insane\n","1225 inside\n","1226 insight\n","1227 insinuating\n","1228 insisted\n","1229 insists\n","1230 inspector\n","1231 Instagram\n","1232 insurance\n","1233 Intelligence\n","1234 intelligence\n","1235 intend\n","1236 intends\n","1237 intercourse\n","1238 interest\n","1239 interested\n","1240 interesting\n","1241 interests\n","1242 interference\n","1243 internal\n","1244 interracial\n","1245 interview\n","1246 interviewed\n","1247 intimidated\n","1248 into\n","1249 introduce\n","1250 invaders\n","1251 investigation\n","1252 investigations\n","1253 involved\n","1254 involvement\n","1255 involving\n","1256 Iowa\n","1257 Iran\n","1258 Iraq\n","1259 Iraqi\n","1260 irredeemable\n","1261 irrevocable\n","1262 IS\n","1263 Is\n","1264 is\n","1265 ISIS\n","1266 Islam\n","1267 Islamic\n","1268 Islamophobic\n","1269 Israel\n","1270 Israelites\n","1271 issue\n","1272 issued\n","1273 It\n","1274 it\n","1275 items\n","1276 ITS\n","1277 its\n","1278 Jack\n","1279 jail\n","1280 Jamal\n","1281 Jarrett\n","1282 jeering\n","1283 Jeez\n","1284 Jeffrey\n","1285 jeopardy\n","1286 Jew\n","1287 job\n","1288 jobs\n","1289 JOHN\n","1290 John\n","1291 joined\n","1292 Joining\n","1293 joining\n","1294 joins\n","1295 joint\n","1296 Jones\n","1297 Journal\n","1298 Journal-Constitution\n","1299 Journalism\n","1300 Joy\n","1301 Juan\n","1302 Juanita\n","1303 Judge\n","1304 judge\n","1305 judgment\n","1306 Judiciary\n","1307 Judy\n","1308 July\n","1309 jurisdictions\n","1310 jury\n","1311 Jussie\n","1312 Just\n","1313 just\n","1314 Justice\n","1315 justice\n","1316 Justin\n","1317 Kaepernick\n","1318 Kaine\n","1319 Kamala\n","1320 Karine\n","1321 Kavanaugh\n","1322 keep\n","1323 Keith\n","1324 Ken\n","1325 key\n","1326 Khashoggi\n","1327 kid\n","1328 kids\n","1329 Kilimnik\n","1330 kill\n","1331 killed\n","1332 killing\n","1333 Kim\n","1334 Kimmel\n","1335 kind\n","1336 kindest\n","1337 kinds\n","1338 Kitty\n","1339 Klan\n","1340 Klansmen\n","1341 Klobuchar\n","1342 know\n","1343 knowing\n","1344 knowledge\n","1345 knows\n","1346 Konstantin\n","1347 Korea\n","1348 Kurtis\n","1349 L.A\n","1350 labor\n","1351 lack\n","1352 laid\n","1353 land\n","1354 landmark\n","1355 language\n","1356 lanky\n","1357 largely\n","1358 larger\n","1359 largest\n","1360 Larry\n","1361 LAST\n","1362 Last\n","1363 last\n","1364 Later\n","1365 latest\n","1366 LAUGHTER\n","1367 launched\n","1368 Laura\n","1369 law\n","1370 lawmakers\n","1371 LAWRENCE\n","1372 Lawrence\n","1373 lawsuit\n","1374 lawsuits\n","1375 lawyer\n","1376 lawyered\n","1377 Lawyers\n","1378 lawyers\n","1379 laying\n","1380 Leader\n","1381 leader\n","1382 leaders\n","1383 leadership\n","1384 league\n","1385 leaving\n","1386 led\n","1387 leeway\n","1388 Left\n","1389 left\n","1390 leg\n","1391 legacy\n","1392 legal\n","1393 legally\n","1394 legislation\n","1395 Legislative\n","1396 legislative\n","1397 legislature\n","1398 legitimate\n","1399 Leno\n","1400 lent\n","1401 Let\n","1402 let\n","1403 levels\n","1404 LGBT\n","1405 liar\n","1406 libel\n","1407 liberal\n","1408 license\n","1409 lie\n","1410 lied\n","1411 lies\n","1412 Lieutenant\n","1413 lieutenant\n","1414 Life\n","1415 life\n","1416 life-and-death\n","1417 life-death\n","1418 like\n","1419 limbs\n","1420 limelight\n","1421 limited\n","1422 Lin\n","1423 lines\n","1424 linked\n","1425 Lisa\n","1426 listen\n","1427 literally\n","1428 litig\n","1429 little\n","1430 ll\n","1431 LLC\n","1432 loan\n","1433 loathsome\n","1434 local\n","1435 London\n","1436 long\n","1437 longtime\n","1438 Look\n","1439 look\n","1440 looking\n","1441 looped\n","1442 Los\n","1443 losers\n","1444 lot\n","1445 lots\n","1446 love\n","1447 lower\n","1448 Luther\n","1449 lying\n","1450 Lynne\n","1451 m\n","1452 MA\n","1453 Ma\n","1454 MacNeil/Lehrer\n","1455 Mad\n","1456 MADDOW\n","1457 Maddow\n","1458 made\n","1459 madman\n","1460 Maduro\n","1461 MAGA\n","1462 MAGA-inspired\n","1463 Magnate\n","1464 mainstream\n","1465 Majority\n","1466 majority\n","1467 Make\n","1468 make\n","1469 making\n","1470 males\n","1471 mama\n","1472 Man\n","1473 man\n","1474 Manafort\n","1475 Manhattan\n","1476 manpower\n","1477 many\n","1478 March\n","1479 Maria\n","1480 Mark\n","1481 market\n","1482 married\n","1483 Massachusetts\n","1484 material\n","1485 materials\n","1486 Matt\n","1487 Matter\n","1488 matter\n","1489 matters\n","1490 Mattes\n","1491 MAY\n","1492 may\n","1493 Maybe\n","1494 maybe\n","1495 Mayor\n","1496 mayor\n","1497 McAuliffe\n","1498 McCabe\n","1499 McQuade\n","1500 Me\n","1501 me\n","1502 mea\n","1503 mean\n","1504 mean-spirited\n","1505 means\n","1506 meant\n","1507 Meanwhile\n","1508 measly\n","1509 measure\n","1510 Media\n","1511 media\n","1512 Medicaid\n","1513 Medicare\n","1514 mediocre\n","1515 meeting\n","1516 meetings\n","1517 member\n","1518 members\n","1519 memo\n","1520 men\n","1521 mental\n","1522 mentioned\n","1523 Michael\n","1524 microphone\n","1525 middle\n","1526 might\n","1527 migrant\n","1528 MIKE\n","1529 Mike\n","1530 miles\n","1531 military\n","1532 million\n","1533 Minister\n","1534 minister\n","1535 Minnesota\n","1536 minors\n","1537 minstrelsy\n","1538 minute\n","1539 Minutes\n","1540 minutes\n","1541 misconduct\n","1542 missile\n","1543 mission\n","1544 mob\n","1545 mock\n","1546 mockery\n","1547 model\n","1548 molested\n","1549 momentous\n","1550 Moms\n","1551 money\n","1552 monsters\n","1553 Month\n","1554 month\n","1555 more\n","1556 morning\n","1557 Moscow\n","1558 most\n","1559 mother\n","1560 Mothers\n","1561 mouth\n","1562 move\n","1563 moved\n","1564 movement\n","1565 movie\n","1566 Mr.\n","1567 Ms.\n","1568 MSNBC\n","1569 Much\n","1570 much\n","1571 much-publicized\n","1572 muck\n","1573 muckety\n","1574 Mueller\n","1575 mugged\n","1576 murdered\n","1577 music\n","1578 Muslim\n","1579 my\n","1580 myself\n","1581 mystery\n","1582 N-\n","1583 n't\n","1584 NAACP\n","1585 Nadler\n","1586 nail\n","1587 named\n","1588 Nancy\n","1589 narrative\n","1590 Nasdaq\n","1591 Nash\n","1592 nation-state\n","1593 National\n","1594 NATO\n","1595 natural\n","1596 Nawaz\n","1597 NBC\n","1598 NEAL\n","1599 near\n","1600 nearly\n","1601 Nebraska\n","1602 necessarily\n","1603 necessary\n","1604 need\n","1605 needed\n","1606 needless\n","1607 negate\n","1608 neither\n","1609 net\n","1610 Netanyahu\n","1611 never\n","1612 never-Trumpers\n","1613 New\n","1614 new\n","1615 Newark\n","1616 News\n","1617 news\n","1618 newscasters\n","1619 NewsHour\n","1620 newspaper\n","1621 next\n","1622 NFL\n","1623 Nicholas\n","1624 Nick\n","1625 nickname\n","1626 Nicolas\n","1627 night\n","1628 Nixon\n","1629 No\n","1630 no\n","1631 nobody\n","1632 nominated\n","1633 nomination\n","1634 nominee\n","1635 non-\n","1636 non-prosecution\n","1637 Nonproliferation\n","1638 nooses\n","1639 nor\n","1640 North\n","1641 Northern\n","1642 nose\n","1643 NOT\n","1644 Not\n","1645 not\n","1646 note\n","1647 nothing\n","1648 notice\n","1649 novel\n","1650 November\n","1651 Now\n","1652 now\n","1653 NPR\n","1654 nuclear\n","1655 nuke\n","1656 Number\n","1657 number\n","1658 numbers\n","1659 O'DONNELL\n","1660 Obama\n","1661 Obama's\n","1662 object\n","1663 obscene\n","1664 obsessions\n","1665 obsessive-compulsive\n","1666 obsolete\n","1667 obtain\n","1668 obtained\n","1669 obvious\n","1670 obviously\n","1671 Ocasio-Cortez\n","1672 October\n","1673 of\n","1674 off\n","1675 offensive\n","1676 offer\n","1677 offered\n","1678 offering\n","1679 Office\n","1680 office\n","1681 officer\n","1682 official\n","1683 officials\n","1684 often\n","1685 Oftentimes\n","1686 Oh\n","1687 oh\n","1688 Ohr\n","1689 OK\n","1690 OK.\n","1691 older\n","1692 Olympic\n","1693 On\n","1694 on\n","1695 one\n","1696 One-point-three\n","1697 ongoing\n","1698 online\n","1699 only\n","1700 open\n","1701 openly\n","1702 opens\n","1703 operating\n","1704 opposition\n","1705 oppressing\n","1706 option\n","1707 options\n","1708 or\n","1709 orchestrated\n","1710 order\n","1711 ordered\n","1712 OREN\n","1713 organ\n","1714 Organization\n","1715 organization\n","1716 organized\n","1717 original\n","1718 originalists\n","1719 Oscar\n","1720 other\n","1721 Ouch\n","1722 Our\n","1723 our\n","1724 out\n","1725 outcome\n","1726 outrageous\n","1727 outside\n","1728 outweigh\n","1729 ovations\n","1730 over\n","1731 overall\n","1732 overnight\n","1733 overseeing\n","1734 Oversight\n","1735 oversight\n","1736 overzealous\n","1737 own\n","1738 oxygen\n","1739 P\n","1740 p.m.\n","1741 PAC\n","1742 Page\n","1743 page\n","1744 pages\n","1745 paid\n","1746 painful\n","1747 Pakistan\n","1748 parallel\n","1749 pardon\n","1750 parents\n","1751 Parkland\n","1752 Parks\n","1753 Parliament\n","1754 part\n","1755 particularly\n","1756 parties\n","1757 partner\n","1758 partners\n","1759 parts\n","1760 Party\n","1761 party\n","1762 pass\n","1763 past\n","1764 pat\n","1765 pathetic\n","1766 patriarchal\n","1767 Patriots\n","1768 Patrol\n","1769 Paul\n","1770 pawn\n","1771 payment\n","1772 payments\n","1773 PBS\n","1774 peace\n","1775 pedophiles\n","1776 Pelosi\n","1777 penalty\n","1778 Pence\n","1779 pending\n","1780 Pentagon\n","1781 People\n","1782 people\n","1783 percent\n","1784 perhaps\n","1785 period\n","1786 perjurer\n","1787 permission\n","1788 person\n","1789 personal\n","1790 perspective\n","1791 ph\n","1792 Pharus\n","1793 philosophers\n","1794 photo\n","1795 photos\n","1796 pick\n","1797 pictures\n","1798 Pigs\n","1799 place\n","1800 places\n","1801 plan\n","1802 planned\n","1803 plans\n","1804 Play\n","1805 play\n","1806 plays\n","1807 plea\n","1808 plead\n","1809 pleaded\n","1810 pleading\n","1811 please\n","1812 pled\n","1813 ploy\n","1814 Plus\n","1815 plus\n","1816 point\n","1817 point-blank\n","1818 points\n","1819 police\n","1820 political\n","1821 politicking\n","1822 polling\n","1823 polls\n","1824 Pompeo\n","1825 POPE\n","1826 Pope\n","1827 pope\n","1828 popular\n","1829 pornography\n","1830 ports\n","1831 possibility\n","1832 possible\n","1833 possibly\n","1834 post\n","1835 potential\n","1836 potentially\n","1837 power\n","1838 Powerful\n","1839 practice\n","1840 praise\n","1841 pre-electronic\n","1842 predictions\n","1843 prepared\n","1844 prescription\n","1845 present\n","1846 presidency\n","1847 President\n","1848 president\n","1849 president's\n","1850 Presidential\n","1851 Presidents\n","1852 presidents\n","1853 press\n","1854 pressing\n","1855 pressure\n","1856 presumably\n","1857 presumption\n","1858 pretty\n","1859 prevent\n","1860 previously\n","1861 price\n","1862 prices\n","1863 priest\n","1864 priests\n","1865 primary\n","1866 Prime\n","1867 printed\n","1868 prior\n","1869 priority\n","1870 prison\n","1871 pro-Clinton\n","1872 probably\n","1873 problem\n","1874 proceed\n","1875 proceedings\n","1876 proceeds\n","1877 process\n","1878 produce\n","1879 Productions\n","1880 professional\n","1881 professor\n","1882 profile\n","1883 Prog\n","1884 prog\n","1885 program\n","1886 programs\n","1887 progress\n","1888 progressive\n","1889 project\n","1890 projects\n","1891 promise\n","1892 promises\n","1893 proposal\n","1894 proposed\n","1895 proprietary\n","1896 prosecution\n","1897 prosecutions\n","1898 prosecutor\n","1899 Prosecutors\n","1900 prosecutors\n","1901 prospect\n","1902 prostitution\n","1903 protected\n","1904 protests\n","1905 proverb\n","1906 provide\n","1907 provided\n","1908 providing\n","1909 proxies\n","1910 psychos\n","1911 publicly\n","1912 published\n","1913 pulling\n","1914 punched\n","1915 punches\n","1916 punk\n","1917 Pure\n","1918 purported\n","1919 purpose\n","1920 purposes\n","1921 pursue\n","1922 pursuing\n","1923 pushing\n","1924 put\n","1925 Putin\n","1926 Putin's\n","1927 puts\n","1928 pyramids\n","1929 queer\n","1930 Question\n","1931 question\n","1932 questions\n","1933 quickly\n","1934 quite\n","1935 Quote\n","1936 quote\n","1937 quote/unquote\n","1938 R\n","1939 Rabid\n","1940 RACHEL\n","1941 Rachel\n","1942 racial\n","1943 Racism\n","1944 racism\n","1945 racist\n","1946 racists\n","1947 radical\n","1948 Rage\n","1949 rain\n","1950 raised\n","1951 raising\n","1952 Ralph\n","1953 Rambo\n","1954 ramming\n","1955 range\n","1956 Rank\n","1957 ranted\n","1958 rape\n","1959 raped\n","1960 rapist\n","1961 rapists\n","1962 rapper\n","1963 rappers\n","1964 rarely\n","1965 rate\n","1966 rats\n","1967 re\n","1968 reach\n","1969 reaction\n","1970 real\n","1971 really\n","1972 reason\n","1973 reasons\n","1974 received\n","1975 recent\n","1976 recognized\n","1977 record\n","1978 recount\n","1979 red\n","1980 redacted\n","1981 reduce\n","1982 reenacted\n","1983 referral\n","1984 reflect\n","1985 refresh\n","1986 regard\n","1987 regular\n","1988 regulations\n","1989 reimbursement\n","1990 reiterated\n","1991 related\n","1992 relations\n","1993 relationship\n","1994 relationships\n","1995 release\n","1996 releasing\n","1997 relevant\n","1998 relief\n","1999 religion\n","2000 religious\n","2001 relitigating\n","2002 remarkable\n","2003 Remember\n","2004 remember\n","2005 remove\n","2006 REP.\n","2007 report\n","2008 reported\n","2009 reportedly\n","2010 reporter\n","2011 reporters\n","2012 Reports\n","2013 Representatives\n","2014 Represented\n","2015 reproduced\n","2016 Republican\n","2017 Republicans\n","2018 repudiate\n","2019 request\n","2020 require\n","2021 required\n","2022 research\n","2023 RESERVED\n","2024 resign\n","2025 resignation\n","2026 resigning\n","2027 resolution\n","2028 response\n","2029 responsibility\n","2030 resulted\n","2031 results\n","2032 retirement\n","2033 return\n","2034 review\n","2035 revolution\n","2036 Richmond\n","2037 ridiculous\n","2038 rig\n","2039 Right\n","2040 right\n","2041 RIGHTS\n","2042 risk\n","2043 Robert\n","2044 Rod\n","2045 Roger\n","2046 role\n","2047 Roll\n","2048 roll\n","2049 Rome\n","2050 room\n","2051 root\n","2052 rose\n","2053 royalty\n","2054 rule\n","2055 ruled\n","2056 rules\n","2057 ruling\n","2058 run\n","2059 runaway\n","2060 running\n","2061 rural\n","2062 RUSH\n","2063 rush\n","2064 rushes\n","2065 Russian\n","2066 Russians\n","2067 Ruth\n","2068 S\n","2069 s\n","2070 Sabato\n","2071 Sad\n","2072 sadly\n","2073 safety\n","2074 said\n","2075 Salisbury\n","2076 same\n","2077 sanctions\n","2078 Sanders\n","2079 Sandmann\n","2080 sandwich\n","2081 Sara\n","2082 Sarah\n","2083 Saturday\n","2084 Saudi\n","2085 savagely\n","2086 say\n","2087 saying\n","2088 says\n","2089 scandal\n","2090 scary\n","2091 scheme\n","2092 Schiff\n","2093 Schifrin\n","2094 schmuck\n","2095 School\n","2096 school\n","2097 schools\n","2098 science-fiction\n","2099 scope\n","2100 Scott\n","2101 screwed\n","2102 SDNY\n","2103 Sean\n","2104 search\n","2105 second\n","2106 seconds\n","2107 secret\n","2108 Secretary\n","2109 secretary\n","2110 Security\n","2111 security\n","2112 See\n","2113 see\n","2114 SEGAL\n","2115 segment\n","2116 selective\n","2117 self-confessed\n","2118 self-defining\n","2119 self-proclaimed\n","2120 selling\n","2121 semitic\n","2122 SEN.\n","2123 send\n","2124 senior\n","2125 sense\n","2126 senselessly\n","2127 sensitive\n","2128 sent\n","2129 sentence\n","2130 sentenced\n","2131 sentencing\n","2132 separated\n","2133 series\n","2134 serious\n","2135 serve\n","2136 served\n","2137 servicemen\n","2138 Services\n","2139 services\n","2140 serving\n","2141 session\n","2142 Sessions\n","2143 sets\n","2144 seven\n","2145 several\n","2146 sex\n","2147 sexist\n","2148 Sexual\n","2149 sexual\n","2150 sexually\n","2151 shall\n","2152 shape\n","2153 share\n","2154 shared\n","2155 Sharon\n","2156 She\n","2157 she\n","2158 Shields\n","2159 shifty\n","2160 shooting\n","2161 Should\n","2162 should\n","2163 SHOW\n","2164 Show\n","2165 show\n","2166 shutdown\n","2167 sick\n","2168 side\n","2169 signed\n","2170 significant\n","2171 silent\n","2172 silly\n","2173 simply\n","2174 Since\n","2175 sincere\n","2176 single\n","2177 single-payer\n","2178 sinker\n","2179 sir\n","2180 sit\n","2181 site\n","2182 sitting\n","2183 size\n","2184 slandered\n","2185 slavery\n","2186 sleet\n","2187 slurs\n","2188 small\n","2189 smear\n","2190 smears\n","2191 smiled\n","2192 Smollett\n","2193 smug\n","2194 snow\n","2195 snuffed\n","2196 So\n","2197 so\n","2198 social\n","2199 socialism\n","2200 socialist\n","2201 society\n","2202 sodomized\n","2203 sole\n","2204 solve\n","2205 Some\n","2206 some\n","2207 somehow\n","2208 something\n","2209 sometimes\n","2210 somewhat\n","2211 son\n","2212 soon\n","2213 soon-to-be\n","2214 sorry\n","2215 sort\n","2216 sorts\n","2217 source\n","2218 South\n","2219 Southern\n","2220 Spartacus\n","2221 speaker\n","2222 speaks\n","2223 Special\n","2224 special\n","2225 specific\n","2226 specifically\n","2227 speech\n","2228 spend\n","2229 spigot\n","2230 Spike\n","2231 Spiro\n","2232 split\n","2233 spoke\n","2234 spoken\n","2235 spray\n","2236 standard\n","2237 stands\n","2238 start\n","2239 started\n","2240 starting\n","2241 starts\n","2242 State\n","2243 statehouse\n","2244 statement\n","2245 statements\n","2246 States\n","2247 states\n","2248 status\n","2249 Stay\n","2250 stay\n","2251 steel\n","2252 Steele\n","2253 steering\n","2254 Step\n","2255 steps\n","2256 stereotype\n","2257 stick\n","2258 Still\n","2259 still\n","2260 stir\n","2261 stocked\n","2262 Stone\n","2263 stop\n","2264 stories\n","2265 storm\n","2266 story\n","2267 straight\n","2268 strange\n","2269 Street\n","2270 Strzok\n","2271 student\n","2272 students\n","2273 studios\n","2274 study\n","2275 stuff\n","2276 stunning\n","2277 stupid\n","2278 stupidest\n","2279 stupidity\n","2280 subject\n","2281 submission\n","2282 subpoena\n","2283 subpoenaed\n","2284 subpoenas\n","2285 substantial\n","2286 substantive\n","2287 succeed\n","2288 success\n","2289 successful\n","2290 suck\n","2291 sucks\n","2292 suddenly\n","2293 suggested\n","2294 suggests\n","2295 summary\n","2296 summer\n","2297 summit\n","2298 Sunday\n","2299 Super\n","2300 superior\n","2301 Superpower\n","2302 support\n","2303 supported\n","2304 supporters\n","2305 supports\n","2306 supposed\n","2307 supposedly\n","2308 supremacist\n","2309 Sure\n","2310 sure\n","2311 surface\n","2312 surgery\n","2313 surmise\n","2314 surprised\n","2315 surprising\n","2316 surrounded\n","2317 Survey\n","2318 Survivors\n","2319 survivors\n","2320 swamp\n","2321 sworn\n","2322 Syria\n","2323 system\n","2324 table\n","2325 Take\n","2326 take\n","2327 takers\n","2328 takes\n","2329 talk\n","2330 talked\n","2331 talking\n","2332 talks\n","2333 Tam\n","2334 Tamara\n","2335 tampering\n","2336 tape\n","2337 tarnishing\n","2338 tax\n","2339 taxes\n","2340 teacher\n","2341 teachers\n","2342 team\n","2343 technology\n","2344 tee\n","2345 teen\n","2346 teenager\n","2347 telegraphs\n","2348 Tell\n","2349 tell\n","2350 telling\n","2351 tells\n","2352 temporary\n","2353 ten\n","2354 Tens\n","2355 terms\n","2356 territory\n","2357 terror\n","2358 terrorism\n","2359 terrorists\n","2360 test\n","2361 testify\n","2362 testifying\n","2363 testimony\n","2364 testing\n","2365 Texas\n","2366 text\n","2367 than\n","2368 Thank\n","2369 thank\n","2370 Thanks\n","2371 thanks\n","2372 That\n","2373 that\n","2374 THE\n","2375 The\n","2376 the\n","2377 thee\n","2378 their\n","2379 them\n","2380 themselves\n","2381 Then\n","2382 then\n","2383 then-\n","2384 There\n","2385 there\n","2386 thereafter\n","2387 these\n","2388 They\n","2389 they\n","2390 they're\n","2391 thing\n","2392 things\n","2393 think\n","2394 thinking\n","2395 thinks\n","2396 THIS\n","2397 This\n","2398 this\n","2399 those\n","2400 though\n","2401 thought\n","2402 thousands\n","2403 thrash\n","2404 threat\n","2405 three\n","2406 throat\n","2407 through\n","2408 throughout\n","2409 thugs\n","2410 thus\n","2411 Till\n","2412 time\n","2413 Times\n","2414 times\n","2415 tip\n","2416 tirade\n","2417 to\n","2418 Today\n","2419 today\n","2420 toddler\n","2421 together\n","2422 told\n","2423 tolerant\n","2424 tomorrow\n","2425 ton\n","2426 tonight\n","2427 too\n","2428 topic\n","2429 touched\n","2430 tough\n","2431 trademark\n","2432 trafficking\n","2433 traitors\n","2434 TRANSCRIPT\n","2435 transcript\n","2436 Transcription\n","2437 transcription\n","2438 transforming\n","2439 transition\n","2440 translator\n","2441 transmitted\n","2442 trashing\n","2443 treasonous\n","2444 treaty\n","2445 trial\n","2446 tricks\n","2447 tricky\n","2448 trillion\n","2449 troops\n","2450 TRUE\n","2451 TRUMP\n","2452 Trump\n","2453 trust\n","2454 truth\n","2455 truth-telling\n","2456 try\n","2457 trying\n","2458 tumor\n","2459 Turned\n","2460 turned\n","2461 turning\n","2462 TV\n","2463 two\n","2464 U.K.\n","2465 U.S\n","2466 U.S.\n","2467 U.S.-backed\n","2468 ugliest\n","2469 ultimately\n","2470 unabated\n","2471 unapologetically\n","2472 unconscionable\n","2473 uncorroborated\n","2474 under\n","2475 undermine\n","2476 understand\n","2477 understanding\n","2478 understands\n","2479 undress\n","2480 uni-Doofus\n","2481 Uninformed\n","2482 unique\n","2483 United\n","2484 universal\n","2485 University\n","2486 unreported\n","2487 unsuccessful\n","2488 until\n","2489 unusual\n","2490 unvetted\n","2491 up\n","2492 UPDATED\n","2493 upon\n","2494 upstate\n","2495 us\n","2496 USA\n","2497 use\n","2498 used\n","2499 user\n","2500 usually\n","2501 utilizing\n","2502 value\n","2503 Values\n","2504 vandalized\n","2505 Vann\n","2506 various\n","2507 Vatican\n","2508 ve\n","2509 Venezuela\n","2510 Venezuelans\n","2511 verbal\n","2512 Vermont\n","2513 version\n","2514 very\n","2515 veto\n","2516 Vice\n","2517 vice\n","2518 Victim\n","2519 victims\n","2520 VIDEO\n","2521 video\n","2522 videotaped\n","2523 videotaping\n","2524 Vietnam\n","2525 view\n","2526 views\n","2527 Vilified\n","2528 Villain\n","2529 violated\n","2530 violence\n","2531 violent\n","2532 VIRGINIA\n","2533 Virginia\n","2534 virulent\n","2535 voices\n","2536 voila\n","2537 voter\n","2538 voters\n","2539 votes\n","2540 wait\n","2541 waiting\n","2542 wake\n","2543 walk\n","2544 Wall\n","2545 wall\n","2546 walled\n","2547 Walter\n","2548 want\n","2549 wanted\n","2550 wants\n","2551 War\n","2552 war\n","2553 Warhol\n","2554 Warner\n","2555 Was\n","2556 was\n","2557 Washington\n","2558 Watch\n","2559 watch\n","2560 watching\n","2561 Watergate\n","2562 wave\n","2563 way\n","2564 ways\n","2565 We\n","2566 we\n","2567 wealth\n","2568 weapons\n","2569 wear\n","2570 wearing\n","2571 Web\n","2572 Wednesday\n","2573 week\n","2574 weekly\n","2575 weeks\n","2576 weird\n","2577 Weisselberg\n","2578 Welcome\n","2579 welcome\n","2580 Well\n","2581 well\n","2582 went\n","2583 were\n","2584 West\n","2585 west\n","2586 Western\n","2587 WFLD\n","2588 What\n","2589 what\n","2590 whatever\n","2591 When\n","2592 when\n","2593 where\n","2594 wherever\n","2595 Whether\n","2596 whether\n","2597 whew\n","2598 which\n","2599 while\n","2600 Whitaker\n","2601 White\n","2602 white\n","2603 who\n","2604 whole\n","2605 whom\n","2606 whose\n","2607 Why\n","2608 why\n","2609 Wilder\n","2610 Will\n","2611 will\n","2612 William\n","2613 willing\n","2614 win\n","2615 wing\n","2616 winner\n","2617 Wisconsin\n","2618 wish\n","2619 WITH\n","2620 with\n","2621 withdraw\n","2622 within\n","2623 without\n","2624 witness\n","2625 witnesses\n","2626 wo\n","2627 woman\n","2628 women\n","2629 Wood\n","2630 Woodruff\n","2631 WORD\n","2632 word\n","2633 words\n","2634 wore\n","2635 work\n","2636 workers\n","2637 working\n","2638 world\n","2639 worse\n","2640 worth\n","2641 would\n","2642 Wow\n","2643 wrap\n","2644 wrapping\n","2645 writer\n","2646 writing\n","2647 written\n","2648 wrong\n","2649 X\n","2650 xenophobe\n","2651 xenophobic\n","2652 Yamiche\n","2653 Yeah\n","2654 year\n","2655 yearbook\n","2656 years\n","2657 yelling\n","2658 Yemen\n","2659 Yes\n","2660 yes\n","2661 yet\n","2662 yielding\n","2663 York\n","2664 You\n","2665 you\n","2666 young\n","2667 younger\n","2668 your\n","2669 yourself\n","2670 zip\n","2671 \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZrP6VXBVoDsq"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W6KN6OODz1_g"},"source":["## Adversarial (pt.2)"]},{"cell_type":"code","metadata":{"id":"IghrMW4h0DCr"},"source":["import json\n","import requests\n","import time\n","def get_score(text):\n","    api_key = 'AIzaSyDcHE3PPBdKxZFm2a075M_RuIvRL9-ATws'\n","    url = ('https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze' +    \n","                '?key=' + api_key)\n","    data_dict = {\n","        'comment': {'text': text},\n","        'languages': ['en'],\n","        'requestedAttributes': {'TOXICITY': {}}\n","    }\n","    response = requests.post(url=url, data=json.dumps(data_dict))\n","    # print(str(response.content))    \n","    response_dict = json.loads(response.content.decode('utf-8'))\n","    # print(text)\n","    # print(response_dict['attributeScores']['TOXICITY']['summaryScore']['value'])\n","    # print(json.dumps(response_dict, indent=2))\n","    return response_dict['attributeScores']['TOXICITY']['summaryScore']['value']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kjfIH7k-z1hM"},"source":["# Adversarial 1 - examples from article and bio - shared by Ani\n","import csv\n","with open('adversarial_5.csv') as f, open('adversarial_5_bert.csv', 'w') as w:\n","    header = f.readline()\n","    reader = csv.reader(f)\n","    writer = csv.writer(w)\n","    for row in reader:\n","        text = row[1]\n","\n","        input = torch.tensor(convert_lines([text], MAX_SEQUENCE_LENGTH, tokenizer), dtype=torch.long)        \n","        pred = model(input.to(device), attention_mask=(input>0).to(device), labels=None)\n","        score = torch.sigmoid(pred).item()\n","\n","        perspective = get_score(text)\n","        writer.writerow([row[0], text, perspective, score])\n","        time.sleep(2)"],"execution_count":null,"outputs":[]}]}