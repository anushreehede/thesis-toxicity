{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Masking.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JHIVoEmGnPke"},"source":["## Test the mask"]},{"cell_type":"code","metadata":{"id":"ogjXsLATpDFS"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ymF7CcgepNv2"},"source":["! pip install -U pytorch-pretrained-bert"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hrI2Q9u-pOnA"},"source":["import torch\n","from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n","\n","# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n","import logging\n","\n","import nltk\n","nltk.download('punkt')\n","\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h6L3cOpBq0KY"},"source":["import csv\n","dir = '/content/drive/My Drive/Backup/Research/Incivility/Perspective/'\n","# Read input data\n","input_file_1 = dir+'Toxicity_Error_Analysis_Train_Set_1_100k.tsv'\n","input_file_2 = dir+'Toxicity_Error_Analysis_Train_Set_2_100k.tsv'\n","# input_file_3 = dir+'Toxicity_Error_Analysis_Train_Set_3_80k.tsv'\n","\n","comments = []\n","scores = []\n","with open(input_file_1) as f:\n","    header = f.readline()\n","    reader = csv.reader(f, delimiter=\"\\t\")\n","    for row in reader:\n","        # dataset.append(row)\n","        new_row = list(filter(('').__ne__, row))\n","        if len(new_row) != 5:\n","            print(new_row[0])\n","        \n","        # print(new_row)\n","        comments.append(new_row[1])\n","        \n","with open(input_file_2) as f:\n","    header = f.readline()\n","    reader = csv.reader(f, delimiter=\"\\t\")\n","    for row in reader:\n","        # dataset.append(row)\n","        new_row = list(filter(('').__ne__, row))\n","        if len(new_row) != 5:\n","            print(new_row)\n","        \n","        comments.append(new_row[1])\n","\n","print(len(comments))\n","# with open(input_file_3) as f:\n","#     header = f.readline()\n","#     reader = csv.reader(f, delimiter=\"\\t\")\n","#     for row in reader:\n","#         # dataset.append(row)\n","#         new_row = list(filter(('').__ne__, row))[:5]\n","#         if len(new_row) != 5:\n","#             print(new_row)\n","#         if float(new_row[4]) < 0:\n","#             comments.append(new_row[1])\n","#             scores.append(float(new_row[4]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zcFbas0nLH7a"},"source":["count = 0\n","cleaned_comments = []\n","for idx, comment in enumerate(comments):\n","\n","    if idx % 100000 == 0:\n","        print(idx)\n","\n","    if comment.count('\\t') > 3:    \n","      # print('\\n Trying to fix ', idx)          \n","      new_lines = comment.split('\\n')\n","      cleaned_comments.append(new_lines[0].split('\\t')[0])\n","      for line in new_lines[1:]:\n","        fields = line.split('\\t')\n","        cleaned_comments.append(fields[1])\n","      count += len(new_lines)\n","    else:\n","      count += 1\n","      cleaned_comments.append(comment)\n","\n","print(count, len(cleaned_comments))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yZSvkuAJrHkV"},"source":["def get_mask_predictions(text):\n","    tokenized_text = tokenizer.tokenize(text)\n","    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)[:512]\n","    id_of_mask = tokenizer.convert_tokens_to_ids(['[MASK]'])[0]\n","    masked_index = indexed_tokens.index(id_of_mask)\n","    # print(indexed_tokens, tokenized_text)\n","\n","    # Create the segments tensors.\n","    segments_ids = [0] * len(tokenized_text)\n","\n","    # Convert inputs to PyTorch tensors\n","    tokens_tensor = torch.tensor([indexed_tokens])\n","    segments_tensors = torch.tensor([segments_ids])\n","\n","    model.eval()\n","\n","    # Predict all tokens\n","    with torch.no_grad():\n","        predictions = model(tokens_tensor, segments_tensors)\n","\n","    predicted_indices = torch.topk(predictions[0, masked_index], 5)[1]\n","    predicted_tokens = [tokenizer.convert_ids_to_tokens([index.item()])[0] for index in predicted_indices]\n","\n","    return predicted_tokens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"81gLy-FEpO5f"},"source":["logging.basicConfig(level=logging.INFO)\n","\n","# Load pre-trained model tokenizer (vocabulary)\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Load pre-trained model (weights)\n","model = BertForMaskedLM.from_pretrained('bert-base-uncased')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hQxcVtpHqaSt"},"source":["error_triggers = ['black', 'dictator', 'police', 'supremacists', 'supremacist', 'gays', 'female', 'blacks', 'racists', 'woman', 'sexist', 'african', 'criminals', 'jewish', 'catholics', 'terrorist', 'fascist', 'muslim', 'white', 'racist', 'terrorists', 'jews', 'women', 'gay', 'muslims']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b32_76z1TdVs"},"source":["cleaned_comments_with_error_trigger = {}\n","cleaned_comments_input_ids = {}\n","for idx, comment in enumerate(cleaned_comments):\n","\n","    if idx % 10000 == 0:\n","        print(idx)\n","\n","    words = nltk.word_tokenize(comment.lower())\n","    for i, word in enumerate(words):\n","        if word in error_triggers:\n","          \n","          if idx in cleaned_comments_with_error_trigger.keys():\n","            cleaned_comments_with_error_trigger[idx].append(word)\n","          else:\n","            cleaned_comments_with_error_trigger[idx] = [word]\n","\n","print(\"*\", len(cleaned_comments_with_error_trigger.keys()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dt8DWOeBspSL"},"source":["error_replacement = {}\n","\n","for counter, index in enumerate(sorted(cleaned_comments_with_error_trigger.keys())):\n","\n","    if counter % 100 == 0:\n","        print(counter)\n","\n","        np.save('/content/drive/My Drive/Incivility/Perspective/Perspective_Error_Analysis/masked_analysis/error_'+str(counter)+'.npy', error_replacement)\n","\n","\n","    comment = cleaned_comments[index]\n","    comment_error_words = cleaned_comments_with_error_trigger[index]\n","\n","    words = nltk.word_tokenize(comment.lower())\n","    \n","    for error in comment_error_words:\n","        \n","        # Mask the word in the comment, lowercase the comment, add the [CLS] and [SEP]\n","        i = words.index(error)\n","        words[i] = '[MASK]'\n","        text = ' '.join(words)\n","        # Get the predictions for the masked word\n","        predictions = get_mask_predictions('[CLS] '+text+' [SEP]')\n","\n","        if error not in error_replacement.keys():\n","            error_replacement[error] = predictions\n","        else:\n","            current = error_replacement[error]\n","            for pred in predictions:\n","              if pred not in current:\n","                current.append(pred)\n","            error_replacement[error] = current"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lc9FKgjXQMPo"},"source":["np.save('/content/drive/My Drive/Incivility/Perspective/Perspective_Error_Analysis/masked_analysis/error_'+str(counter)+'.npy', error_replacement)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bGHqLqtGQci8"},"source":["for error in error_replacement.keys():\n","    print(error + \": \" + str(len(error_replacement[error])))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qv27LHShQrjF"},"source":["file = '/content/drive/My Drive/Incivility/Perspective/Perspective_Error_Analysis/error_trigger_densities/all_errors/Templates.csv'\n","\n","import csv\n","\n","with open (file) as f:\n","    header = f.readline()\n","    reader = csv.reader(f)\n","    all_error_triggers = [row[0] for row in reader]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LbNX4VjZRuNA"},"source":["import random\n","for error in error_replacement.keys():\n","    c=0\n","    non_error = []\n","    for replace in error_replacement[error]:\n","        if replace in all_error_triggers:\n","            c += 1\n","        else:\n","            non_error.append(replace)\n","    l = len(error_replacement[error])\n","    print(error + \": %d, percent of replacements are error triggers: %.2f\" % (l, float(c) / l * 100))\n","    print('\\t' + str(random.sample(non_error, 10)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ym1xp37-nWB8"},"source":["## Replace mask with word"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i14LyEMc_YVj","executionInfo":{"status":"ok","timestamp":1619610327695,"user_tz":240,"elapsed":546083,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}},"outputId":"60c8dd59-8166-492d-b889-8e884897660d"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JfZooOZK_YV1","executionInfo":{"status":"ok","timestamp":1619610336166,"user_tz":240,"elapsed":554536,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}},"outputId":"a36180f7-2a1f-43ac-8d04-8c57734d3c09"},"source":["! pip install -U pytorch-pretrained-bert"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting pytorch-pretrained-bert\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n","\r\u001b[K     |██▋                             | 10kB 13.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 19.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 11.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 9.2MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 5.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 6.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 6.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 6.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 6.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 6.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 6.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 6.4MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n","Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n","Collecting boto3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/a3/388f78a63b0545f30daf13e6542b0d850f621ff51ef721cf431eef609c87/boto3-1.17.59-py2.py3-none-any.whl (131kB)\n","\u001b[K     |████████████████████████████████| 133kB 8.8MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n","Requirement already satisfied, skipping upgrade: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.8.1+cu101)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n","Collecting jmespath<1.0.0,>=0.7.1\n","  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n","Collecting botocore<1.21.0,>=1.20.59\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/68/e91883d8f68183216fac7cfa855cd0d1b3336c3d4b82cd4b99591118bcc7/botocore-1.20.59-py2.py3-none-any.whl (7.4MB)\n","\u001b[K     |████████████████████████████████| 7.4MB 9.1MB/s \n","\u001b[?25hCollecting s3transfer<0.5.0,>=0.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n","\u001b[K     |████████████████████████████████| 81kB 8.7MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2020.12.5)\n","Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.59->boto3->pytorch-pretrained-bert) (2.8.1)\n","Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.59->boto3->pytorch-pretrained-bert) (1.15.0)\n","\u001b[31mERROR: botocore 1.20.59 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n","Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n","Successfully installed boto3-1.17.59 botocore-1.20.59 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.4.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YTpQcNh9_YV1","executionInfo":{"status":"ok","timestamp":1619610339481,"user_tz":240,"elapsed":557835,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}},"outputId":"97e7ac1a-b96e-4305-a3bf-fe87070c2ee5"},"source":["import torch\n","from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n","\n","# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n","import logging\n","\n","import nltk\n","nltk.download('punkt')\n","\n","import numpy as np\n","\n","import csv"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aL2dy7W0CiGC","executionInfo":{"status":"ok","timestamp":1619610339482,"user_tz":240,"elapsed":14139,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}}},"source":["# Overpredictions and underpredictions\n","\n","def make_clean_dataset(dataset):\n","    count = 0\n","    cleaned_dataset = []\n","    for idx, line in enumerate(dataset):\n","\n","        if idx % 100000 == 0:\n","            print(idx)\n","\n","        comment, remaining = line[1], line[2:]\n","\n","        if comment.count('\\t') > 3:    \n","            # print('\\n Trying to fix **', line)          \n","            new_lines = comment.split('\\n')\n","            \n","            first_row = new_lines[0].split('\\t')\n","            first_row.insert(0, line[0])\n","            cleaned_dataset.append(first_row)\n","            # print('^ ', first_row)\n","            for linex in new_lines[1:-1]:\n","                fields = linex.split('\\t')\n","                # print('& ', fields)\n","                cleaned_dataset.append(fields)\n","            \n","            last_row = new_lines[-1].split('\\t')\n","            last_row.extend(remaining)\n","            cleaned_dataset.append(last_row)\n","            # print('$ ', last_row)\n","            count += len(new_lines)\n","            # break\n","        else:\n","            count += 1\n","            cleaned_dataset.append(line)\n","\n","    print(count, len(cleaned_dataset))\n","    return cleaned_dataset"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"43lzmcPN_YV2","executionInfo":{"status":"ok","timestamp":1619610345910,"user_tz":240,"elapsed":18636,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}},"outputId":"f18388ad-9ccd-4721-ec8e-379d2e2138fc"},"source":["input_file_1 = '/content/drive/My Drive/Backup/Research/Incivility/Perspective/Toxicity_Error_Analysis_Train_Set_1_100k.tsv'\n","\n","dataset1 = []\n","with open(input_file_1) as f:\n","    header = f.readline()\n","    reader = csv.reader(f, delimiter=\"\\t\")\n","    for i, row in enumerate(reader):\n","\n","        if i%10000 == 0:\n","            print(i)\n","        \n","        # if i == 100:\n","        #     break\n","        new_row = list(filter(('').__ne__, row))[:5]\n","        # if len(new_row) != 5:\n","        #     print(new_row[0])\n","        dataset1.append(new_row)\n","\n","    cleaned_dataset1 = make_clean_dataset(dataset1)\n","    \n","input_file_2 = '/content/drive/My Drive/Backup/Research/Incivility/Perspective/Toxicity_Error_Analysis_Train_Set_2_100k.tsv'\n","\n","dataset2 = []\n","with open(input_file_2) as f:\n","    header = f.readline()\n","    reader = csv.reader(f, delimiter=\"\\t\")\n","    for i, row in enumerate(reader):\n","\n","        if i%10000 == 0:\n","            print(i)\n","        \n","        # if i == 100:\n","        #     break\n","        new_row = list(filter(('').__ne__, row))[:5]\n","        # if len(new_row) != 5:\n","        #     print(new_row[0])\n","        dataset2.append(new_row)\n","\n","    cleaned_dataset2 = make_clean_dataset(dataset2)\n","\n","input_file_3 = '/content/drive/My Drive/Backup/Research/Incivility/Perspective/Toxicity_Error_Analysis_Train_Set_3_100k.tsv'\n","\n","dataset3 = []\n","with open(input_file_3) as f:\n","    header = f.readline()\n","    reader = csv.reader(f, delimiter=\"\\t\")\n","    for i, row in enumerate(reader):\n","\n","        if i%10000 == 0:\n","            print(i)\n","        \n","        # if i == 100:\n","        #     break\n","        new_row = list(filter(('').__ne__, row))[:5]\n","        # if len(new_row) != 5:\n","        #     print(new_row[0])\n","        dataset3.append(new_row)\n","\n","    cleaned_dataset3 = make_clean_dataset(dataset3)\n","\n","dataset = []\n","dataset.extend(cleaned_dataset1)\n","dataset.extend(cleaned_dataset2)\n","dataset.extend(cleaned_dataset3)\n","\n","print(len(dataset))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["0\n","10000\n","20000\n","30000\n","40000\n","50000\n","60000\n","70000\n","80000\n","90000\n","100000\n","0\n","100000\n","101596 101596\n","0\n","10000\n","20000\n","30000\n","40000\n","50000\n","60000\n","70000\n","80000\n","90000\n","0\n","95510 95510\n","0\n","10000\n","20000\n","30000\n","40000\n","50000\n","60000\n","70000\n","80000\n","90000\n","100000\n","0\n","100000\n","103700 103700\n","300806\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Au4g88r2_YV3","executionInfo":{"status":"ok","timestamp":1619610346640,"user_tz":240,"elapsed":729,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}}},"source":["def get_mask_predictions(text):\n","    tokenized_text = tokenizer.tokenize(text)\n","    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)[:512]\n","    id_of_mask = tokenizer.convert_tokens_to_ids(['[MASK]'])[0]\n","    masked_index = indexed_tokens.index(id_of_mask)\n","    # print(indexed_tokens, tokenized_text)\n","\n","    # Create the segments tensors.\n","    segments_ids = [0] * len(tokenized_text)\n","\n","    # Convert inputs to PyTorch tensors\n","    tokens_tensor = torch.tensor([indexed_tokens])\n","    segments_tensors = torch.tensor([segments_ids])\n","\n","    model.eval()\n","\n","    # Predict all tokens\n","    with torch.no_grad():\n","        predictions = model(tokens_tensor, segments_tensors)\n","\n","    predicted_indices = torch.topk(predictions[0, masked_index], 5)[1]\n","    predicted_tokens = [tokenizer.convert_ids_to_tokens([index.item()])[0] for index in predicted_indices]\n","\n","    return predicted_tokens"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sitIP-eo_YV3","executionInfo":{"status":"ok","timestamp":1619610370162,"user_tz":240,"elapsed":24232,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}},"outputId":"8acd87bd-7b16-4429-ff52-8453426b1806"},"source":["logging.basicConfig(level=logging.INFO)\n","\n","# Load pre-trained model tokenizer (vocabulary)\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","# Load pre-trained model (weights)\n","model = BertForMaskedLM.from_pretrained('bert-base-uncased')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["INFO:pytorch_pretrained_bert.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache, downloading to /tmp/tmpb6ndkxzw\n","100%|██████████| 231508/231508 [00:00<00:00, 5682138.29B/s]\n","INFO:pytorch_pretrained_bert.file_utils:copying /tmp/tmpb6ndkxzw to cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","INFO:pytorch_pretrained_bert.file_utils:creating metadata file for /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","INFO:pytorch_pretrained_bert.file_utils:removing temp file /tmp/tmpb6ndkxzw\n","INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","INFO:pytorch_pretrained_bert.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz not found in cache, downloading to /tmp/tmpntjyr9ur\n","100%|██████████| 407873900/407873900 [00:08<00:00, 49295646.09B/s]\n","INFO:pytorch_pretrained_bert.file_utils:copying /tmp/tmpntjyr9ur to cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","INFO:pytorch_pretrained_bert.file_utils:creating metadata file for /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","INFO:pytorch_pretrained_bert.file_utils:removing temp file /tmp/tmpntjyr9ur\n","INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n","INFO:pytorch_pretrained_bert.modeling:extracting archive file /root/.pytorch_pretrained_bert/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmpqf996x4g\n","INFO:pytorch_pretrained_bert.modeling:Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"type_vocab_size\": 2,\n","  \"vocab_size\": 30522\n","}\n","\n","INFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"CQv8LcZ0_YV3","executionInfo":{"status":"ok","timestamp":1619610370162,"user_tz":240,"elapsed":24216,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}}},"source":["# error_triggers = ['black', 'dictator', 'police', 'supremacists', 'supremacist', 'gays', 'female', 'blacks', 'racists', 'woman', 'sexist', 'african', 'criminals', 'jewish', 'catholics', 'terrorist', 'fascist', 'muslim', 'white', 'racist', 'terrorists', 'jews', 'women', 'gay', 'muslims']\n","error_triggers = ['dictators','refugee','feminist','communist','catholic','migrants','homosexual','babies','mexicans','girl','islamic']"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0GHVBu8w_YV4","executionInfo":{"status":"ok","timestamp":1619610520046,"user_tz":240,"elapsed":174084,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}},"outputId":"f3915f4c-398f-4b4c-9783-df4977a7ace7"},"source":["cleaned_comments_with_error_trigger = {}\n","for idx, row in enumerate(dataset):\n","\n","    if idx % 10000 == 0:\n","        print(idx)\n","    comment = row[1]\n","    index = idx\n","    words = nltk.word_tokenize(comment.lower())\n","    for i, word in enumerate(words):\n","        if word in error_triggers:\n","          \n","          if index in cleaned_comments_with_error_trigger.keys():\n","            cleaned_comments_with_error_trigger[index].append(word)\n","          else:\n","            cleaned_comments_with_error_trigger[index] = [word]\n","\n","print(\"*\", len(cleaned_comments_with_error_trigger.keys()))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["0\n","10000\n","20000\n","30000\n","40000\n","50000\n","60000\n","70000\n","80000\n","90000\n","100000\n","110000\n","120000\n","130000\n","140000\n","150000\n","160000\n","170000\n","180000\n","190000\n","200000\n","210000\n","220000\n","230000\n","240000\n","250000\n","260000\n","270000\n","280000\n","290000\n","300000\n","* 6308\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CmPz8WccEVN8","executionInfo":{"status":"ok","timestamp":1619616205686,"user_tz":240,"elapsed":5670694,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}},"outputId":"66513a20-67a6-41e5-af7d-dcb41ae4c3c5"},"source":["import random\n","\n","replaced_dataset = []\n","\n","replacements_are_errors = {}\n","for e in error_triggers:\n","    replacements_are_errors[e] = {}\n","\n","for counter, index in enumerate(sorted(cleaned_comments_with_error_trigger.keys())):\n","\n","    if counter % 1000 == 0:\n","        print(counter)\n","        # TODO\n","        np.save('/content/drive/My Drive/Incivility/Perspective/Perspective_Error_Analysis/masked_analysis/replaced_data_'+str(counter)+'.npy', replaced_dataset)\n","        np.save('/content/drive/My Drive/Incivility/Perspective/Perspective_Error_Analysis/masked_analysis/replacement_predictions_'+str(counter)+'.npy', replacements_are_errors)\n","\n","\n","    comment = dataset[index][1]\n","    label = float(dataset[index][2])\n","    comment_error_words = cleaned_comments_with_error_trigger[index]\n","\n","    for error in comment_error_words:\n","        \n","        words = nltk.word_tokenize(comment.lower())\n","        # Mask the word in the comment, lowercase the comment, add the [CLS] and [SEP]\n","        i = words.index(error)\n","        words[i] = '[MASK]'\n","        text = ' '.join(words)\n","        # Get the predictions for the masked word\n","        predictions = get_mask_predictions('[CLS] '+text+' [SEP]')\n","\n","        replacements_are_errors[error][index] = predictions\n","\n","        # Choose one of the predictions\n","        replacement = random.choice(predictions)\n","        \n","        # Get the replaced text\n","        replaced_words = [replacement if x == '[MASK]' else x for i, x in enumerate(words)]\n","        replaced_text = ' '.join(replaced_words)\n","        \n","        # TODO - what to do with the label\n","        replaced_dataset.append([replaced_text, label])\n","\n","np.save('/content/drive/My Drive/Incivility/Perspective/Perspective_Error_Analysis/masked_analysis/replaced_data_final.npy', replaced_dataset)\n","np.save('/content/drive/My Drive/Incivility/Perspective/Perspective_Error_Analysis/masked_analysis/replacement_predictions_final.npy', replacements_are_errors)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["0\n","1000\n","2000\n","3000\n","4000\n","5000\n","6000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"egmCCTCdTuvq"},"source":["replacements_are_errors = np.load('/content/drive/My Drive/Incivility/Perspective/Perspective_Error_Analysis/masked_analysis/replacement_predictions.npy', allow_pickle=True).item()\n","\n","for counter, index in enumerate(sorted(cleaned_comments_with_error_trigger.keys())):\n","\n","    if counter % 1000 == 0:\n","        print(counter)\n","        # TODO\n","        # np.save('/content/drive/My Drive/Incivility/Perspective/Perspective_Error_Analysis/masked_analysis/replaced_data_'+str(counter)+'.npy', replaced_dataset)\n","        # np.save('/content/drive/My Drive/Incivility/Perspective/Perspective_Error_Analysis/masked_analysis/replacement_predictions_'+str(counter)+'.npy', replacements_are_errors)\n","\n","    comment = dataset[index][1]\n","    label = float(dataset[index][2])\n","    comment_error_words = cleaned_comments_with_error_trigger[index]\n","\n","    for error in comment_error_words:\n","        \n","        words = nltk.word_tokenize(comment.lower())\n","        # Mask the word in the comment, lowercase the comment, add the [CLS] and [SEP]\n","        i = words.index(error)\n","        words[i] = '[MASK]'\n","\n","        predictions = replacements_are_errors[error][index]\n","        \n","        # Choose one of the predictions\n","        replacement = random.choice(predictions)\n","        \n","        # Get the replaced text\n","        replaced_words = [replacement if x == '[MASK]' else x for i, x in enumerate(words)]\n","        replaced_text = ' '.join(replaced_words)\n","        \n","        # TODO - what to do with the label\n","        replaced_dataset.append([replaced_text, label])\n","\n","np.save('/content/drive/My Drive/Incivility/Perspective/Perspective_Error_Analysis/masked_analysis/replaced_data.npy', replaced_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QFqnKSmFEWVW","executionInfo":{"status":"ok","timestamp":1619216685795,"user_tz":240,"elapsed":5042,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}},"outputId":"dd2f68fd-b72e-482e-be69-3d451f9f3ccb"},"source":["import pandas as pd\n","\n","data = np.load('/content/drive/My Drive/Incivility/Perspective/Perspective_Error_Analysis/masked_analysis/replaced_data.npy')\n","\n","print(len(data))\n","\n","# Save as a pandas file for future work \n","df = pd.DataFrame(data, columns=['comment_text', 'target'])\n","\n","df.to_csv('/content/drive/My Drive/Incivility/Perspective/Perspective_BERT/Retrained/replaced_toxicity.csv')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["50087\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oHXbHgiULXXq","executionInfo":{"status":"ok","timestamp":1619214958196,"user_tz":240,"elapsed":441,"user":{"displayName":"Anushree Hede","photoUrl":"","userId":"15511138661039093070"}},"outputId":"da68b106-ed42-4ce3-ff87-fa9eca6f67ba"},"source":["data[100:103]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([['it is not coincidence that the best candidates are all males it is fact and history proves it . there is a reason white males win more wars , more business , more awards . it is racism . white people are simply better . funny how it really works triggered yet booya',\n","        '0.8'],\n","       ['it is not coincidence that the best candidates are african males it is fact and history proves it . there is a reason white males win more wars , more business , more awards . it is racism . white people are simply better . funny how it really works triggered yet booya',\n","        '0.8'],\n","       ['it is not coincidence that the best candidates are white males it is fact and history proves it . there is a reason white males win more wars , more business , more awards . it is racism . white people are simply better . funny how it really works triggered yet booya',\n","        '0.8']], dtype='<U1901')"]},"metadata":{"tags":[]},"execution_count":14}]}]}